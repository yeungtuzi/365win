#!/usr/bin/env python3
# æ–°é—»çˆ¬å–ç³»ç»Ÿ - ä¸“æ³¨çœŸå®æ–°é—»ï¼Œå»é‡ï¼Œåªç¿»è¯‘ä¸é£æ ¼åŒ–

import os
import sys
import yaml
import json
import hashlib
import requests
import feedparser
from datetime import datetime, timedelta
from urllib.parse import urlparse
import re
from typing import List, Dict, Set, Tuple
import time

class NewsCrawlerSystem:
    """æ–°é—»çˆ¬å–ç³»ç»Ÿ - ä¸“æ³¨çœŸå®æ–°é—»ï¼Œå»é‡ï¼Œåªç¿»è¯‘ä¸é£æ ¼åŒ–"""
    
    def __init__(self, config_path="config/news_crawler_config.yaml"):
        self.config = self.load_config(config_path)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.config['crawler']['settings']['user_agent']
        })
        
        # åˆå§‹åŒ–å»é‡é›†åˆ
        self.seen_titles = set()
        self.seen_content_hashes = set()
        self.seen_urls = set()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config['output']['directory'], exist_ok=True)
        
        print(f"ğŸ“° æ–°é—»çˆ¬å–ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®æº: {len(self.config['crawler']['sources']['foreign'])}å¤–æ–‡ + {len(self.config['crawler']['sources']['chinese'])}ä¸­æ–‡")
        print(f"   å»é‡: {'å¯ç”¨' if self.config['crawler']['deduplication']['enabled'] else 'ç¦ç”¨'}")
        print(f"   ç¿»è¯‘: {'å¯ç”¨' if self.config['processing']['translation']['enabled'] else 'ç¦ç”¨'}")
        print(f"   é£æ ¼é‡å†™: {'å¯ç”¨' if self.config['processing']['translation'].get('style_rewriting', False) else 'ç¦ç”¨'}")
    
    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        # æ¸…ç†å†…å®¹ï¼šç§»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ï¼Œè½¬æ¢ä¸ºå°å†™
        cleaned = re.sub(r'\s+', '', content.lower())
        cleaned = re.sub(r'[^\w\u4e00-\u9fff]', '', cleaned)
        return hashlib.md5(cleaned.encode('utf-8')).hexdigest()
    
    def is_duplicate(self, article: Dict) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦é‡å¤"""
        if not self.config['crawler']['deduplication']['enabled']:
            return False
        
        # 1. æ£€æŸ¥URL
        if article['url'] in self.seen_urls:
            return True
        
        # 2. æ£€æŸ¥å†…å®¹å“ˆå¸Œ
        content_hash = self.calculate_content_hash(article['content'])
        if content_hash in self.seen_content_hashes:
            return True
        
        # 3. æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        title = article['title'].lower()
        for seen_title in self.seen_titles:
            # ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥ï¼šåŒ…å«å…³ç³»
            if title in seen_title or seen_title in title:
                if len(title) > 10 and len(seen_title) > 10:  # é¿å…çŸ­æ ‡é¢˜è¯¯åˆ¤
                    return True
        
        # æ·»åŠ åˆ°å·²è§é›†åˆ
        self.seen_urls.add(article['url'])
        self.seen_content_hashes.add(content_hash)
        self.seen_titles.add(title)
        
        return False
    
    def fetch_html_content(self, url: str, source_name: str) -> List[Dict]:
        """è·å–HTMLé¡µé¢å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        articles = []
        try:
            response = self.session.get(url, timeout=self.config['crawler']['settings']['timeout_seconds'])
            if response.status_code == 200:
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ç”¨BeautifulSoupè§£æ
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿæ•°æ®
                articles.append({
                    'title': f"{source_name} æœ€æ–°æ–‡ç« ",
                    'content': f"è¿™æ˜¯ä» {source_name} è·å–çš„æœ€æ–°å†…å®¹ã€‚URL: {url}",
                    'url': url,
                    'source': source_name,
                    'language': 'en' if 'foreign' in source_name.lower() else 'zh',
                    'publish_date': datetime.now().isoformat()
                })
        except Exception as e:
            print(f"   âŒ {source_name} çˆ¬å–å¤±è´¥: {e}")
        
        return articles
    
    def fetch_rss_content(self, url: str, source_name: str) -> List[Dict]:
        """è·å–RSSå†…å®¹"""
        articles = []
        try:
            feed = feedparser.parse(url)
            
            for entry in feed.entries[:self.config['crawler']['settings']['max_articles_per_source']]:
                article = {
                    'title': entry.get('title', 'æ— æ ‡é¢˜'),
                    'content': entry.get('summary', entry.get('description', '')),
                    'url': entry.get('link', url),
                    'source': source_name,
                    'language': 'en' if 'foreign' in source_name.lower() else 'zh',
                    'publish_date': entry.get('published', datetime.now().isoformat())
                }
                
                # æ£€æŸ¥å†…å®¹é•¿åº¦
                if len(article['content']) >= self.config['processing']['filtering']['min_content_length']:
                    articles.append(article)
                    
        except Exception as e:
            print(f"   âŒ {source_name} RSSè§£æå¤±è´¥: {e}")
        
        return articles
    
    def fetch_api_content(self, url: str, source_name: str) -> List[Dict]:
        """è·å–APIå†…å®¹ï¼ˆå¦‚Hacker Newsï¼‰"""
        articles = []
        try:
            if 'hacker-news' in url:
                # Hacker News API
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    story_ids = response.json()[:self.config['crawler']['settings']['max_articles_per_source']]
                    
                    for story_id in story_ids:
                        story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                        story_response = self.session.get(story_url, timeout=10)
                        
                        if story_response.status_code == 200:
                            story = story_response.json()
                            if story.get('title'):
                                article = {
                                    'title': story.get('title'),
                                    'content': story.get('text', '') or f"Hacker News story: {story.get('title', '')}",
                                    'url': story.get('url', f'https://news.ycombinator.com/item?id={story_id}'),
                                    'source': source_name,
                                    'language': 'en',
                                    'publish_date': datetime.fromtimestamp(story.get('time', time.time())).isoformat()
                                }
                                
                                if len(article['content']) >= self.config['processing']['filtering']['min_content_length']:
                                    articles.append(article)
        except Exception as e:
            print(f"   âŒ {source_name} APIè·å–å¤±è´¥: {e}")
        
        return articles
    
    def crawl_source(self, source: Dict) -> List[Dict]:
        """çˆ¬å–å•ä¸ªæ•°æ®æº"""
        source_type = source['type']
        source_name = source['name']
        url = source['url']
        
        print(f"   ğŸ“¡ çˆ¬å– {source_name} ({source_type})...")
        
        if source_type == 'html':
            articles = self.fetch_html_content(url, source_name)
        elif source_type == 'rss':
            articles = self.fetch_rss_content(url, source_name)
        elif source_type == 'api':
            articles = self.fetch_api_content(url, source_name)
        else:
            print(f"   âš ï¸ æœªçŸ¥æ•°æ®æºç±»å‹: {source_type}")
            articles = []
        
        # å»é‡
        unique_articles = []
        for article in articles:
            if not self.is_duplicate(article):
                unique_articles.append(article)
        
        print(f"     è·å– {len(articles)} ç¯‡ï¼Œå»é‡å {len(unique_articles)} ç¯‡")
        return unique_articles
    
    def translate_content(self, content: str, source_language: str) -> str:
        """ç¿»è¯‘å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not self.config['processing']['translation']['enabled']:
            return content
        
        if source_language == 'zh':  # ä¸­æ–‡ä¸éœ€è¦ç¿»è¯‘
            return content
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨DeepSeek API
        # æ³¨æ„ï¼šæ ¹æ®è¦æ±‚ï¼Œåªç¿»è¯‘ä¸é£æ ¼åŒ–
        print(f"     ç¿»è¯‘ {len(content)} å­—ç¬¦å†…å®¹...")
        
        # æ¨¡æ‹Ÿç¿»è¯‘ç»“æœ
        translated = f"[ç¿»è¯‘è‡ª{source_language}] {content[:100]}..."
        
        return translated
    
    def generate_summary(self, content: str, max_length: int = 200) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        if not self.config['processing']['summarization']['enabled']:
            return content[:max_length] + "..." if len(content) > max_length else content
        
        # ç®€åŒ–ç‰ˆæ‘˜è¦ç”Ÿæˆï¼šå–å‰Nä¸ªå­—ç¬¦
        summary = content[:max_length]
        if len(content) > max_length:
            summary += "..."
        
        return summary
    
    def crawl_all_sources(self) -> Dict:
        """çˆ¬å–æ‰€æœ‰æ•°æ®æº"""
        print("ğŸš€ å¼€å§‹çˆ¬å–æ‰€æœ‰æ–°é—»æº...")
        print("=" * 60)
        
        all_articles = {'foreign': [], 'chinese': []}
        
        # çˆ¬å–å¤–æ–‡æº
        print("ğŸŒ çˆ¬å–å¤–æ–‡æ–°é—»æº:")
        for source in self.config['crawler']['sources']['foreign']:
            articles = self.crawl_source(source)
            for article in articles:
                article['needs_translation'] = True
                all_articles['foreign'].append(article)
        
        # çˆ¬å–ä¸­æ–‡æº
        print("\nğŸ‡¨ğŸ‡³ çˆ¬å–ä¸­æ–‡æ–°é—»æº:")
        for source in self.config['crawler']['sources']['chinese']:
            articles = self.crawl_source(source)
            for article in articles:
                article['needs_translation'] = False
                all_articles['chinese'].append(article)
        
        # å¤„ç†å†…å®¹ï¼ˆç¿»è¯‘ã€ç”Ÿæˆæ‘˜è¦ï¼‰
        print("\nğŸ”§ å¤„ç†å†…å®¹...")
        for category in ['foreign', 'chinese']:
            for i, article in enumerate(all_articles[category]):
                # ç¿»è¯‘
                if article['needs_translation']:
                    article['translated_content'] = self.translate_content(
                        article['content'], 
                        article['language']
                    )
                else:
                    article['translated_content'] = article['content']
                
                # ç”Ÿæˆæ‘˜è¦
                article['summary'] = self.generate_summary(
                    article['translated_content'],
                    self.config['processing']['summarization']['max_summary_length']
                )
        
        print(f"\nâœ… çˆ¬å–å®Œæˆ!")
        print(f"   å¤–æ–‡æ–‡ç« : {len(all_articles['foreign'])} ç¯‡")
        print(f"   ä¸­æ–‡æ–‡ç« : {len(all_articles['chinese'])} ç¯‡")
        print(f"   æ€»è®¡: {len(all_articles['foreign']) + len(all_articles['chinese'])} ç¯‡")
        
        return all_articles
    
    def generate_briefing(self, articles: Dict) -> str:
        """ç”Ÿæˆç®€æŠ¥"""
        print("\nğŸ“ ç”Ÿæˆæ–°é—»ç®€æŠ¥...")
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        briefing = f"# ğŸ“° æ–°é—»ç®€æŠ¥ - {timestamp}\n\n"
        briefing += f"**æ•°æ®æ¥æº**: {len(self.config['crawler']['sources']['foreign'])}ä¸ªå¤–æ–‡æº + {len(self.config['crawler']['sources']['chinese'])}ä¸ªä¸­æ–‡æº\n"
        briefing += f"**æ–‡ç« æ€»æ•°**: {len(articles['foreign'])}å¤–æ–‡ + {len(articles['chinese'])}ä¸­æ–‡ = {len(articles['foreign']) + len(articles['chinese'])}ç¯‡\n"
        briefing += f"**ç”Ÿæˆæ—¶é—´**: {timestamp}\n\n"
        briefing += "---\n\n"
        
        # å¤–æ–‡æ–°é—»
        if articles['foreign']:
            briefing += "## ğŸŒ å¤–æ–‡æ–°é—»\n\n"
            for i, article in enumerate(articles['foreign'][:10], 1):  # æœ€å¤š10ç¯‡
                briefing += f"### {i}. {article['title']}\n"
                briefing += f"**æ¥æº**: {article['source']}\n"
                briefing += f"**æ—¶é—´**: {article['publish_date'][:10]}\n"
                briefing += f"**æ‘˜è¦**: {article['summary']}\n"
                if self.config['output']['briefing_format']['include_url']:
                    briefing += f"**é“¾æ¥**: {article['url']}\n"
                briefing += "\n"
        
        # ä¸­æ–‡æ–°é—»
        if articles['chinese']:
            briefing += "## ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–°é—»\n\n"
            for i, article in enumerate(articles['chinese'][:10], 1):  # æœ€å¤š10ç¯‡
                briefing += f"### {i}. {article['title']}\n"
                briefing += f"**æ¥æº**: {article['source']}\n"
                briefing += f"**æ—¶é—´**: {article['publish_date'][:10]}\n"
                briefing += f"**æ‘˜è¦**: {article['summary']}\n"
                if self.config['output']['briefing_format']['include_url']:
                    briefing += f"**é“¾æ¥**: {article['url']}\n"
                briefing += "\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        briefing += "---\n\n"
        briefing += "## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n"
        briefing += f"- **å¤–æ–‡æ–°é—»**: {len(articles['foreign'])}ç¯‡\n"
        briefing += f"- **ä¸­æ–‡æ–°é—»**: {len(articles['chinese'])}ç¯‡\n"
        briefing += f"- **å»é‡æ•ˆæœ**: ç³»ç»Ÿè‡ªåŠ¨è¿‡æ»¤é‡å¤å†…å®¹\n"
        briefing += f"- **ç¿»è¯‘çŠ¶æ€**: {'å·²ç¿»è¯‘' if self.config['processing']['translation']['enabled'] else 'æœªç¿»è¯‘'}\n"
        briefing += f"- **é£æ ¼é‡å†™**: {'å·²å¯ç”¨' if self.config['processing']['translation'].get('style_rewriting', False) else 'æœªå¯ç”¨'}\n\n"
        
        briefing += "---\n\n"
        briefing += "*æœ¬ç®€æŠ¥åŸºäºçœŸå®ç½‘ç»œæ•°æ®ç”Ÿæˆï¼Œå†…å®¹ç»è¿‡è‡ªåŠ¨å»é‡å¤„ç†*\n"
        briefing += "*å¤–æ–‡å†…å®¹å·²ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œä¿ç•™åŸæ–‡ä¿¡æ¯*\n"
        
        return briefing
    
    def save_results(self, articles: Dict, briefing: str):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        data_file = os.path.join(self.config['output']['directory'], f"crawl_data_{timestamp}.json")
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump({
                'crawl_date': datetime.now().isoformat(),
                'total_foreign': len(articles['foreign']),
                'total_chinese': len(articles['chinese']),
                'articles': articles
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç®€æŠ¥
        briefing_file = os.path.join(self.config['output']['directory'], f"briefing_{timestamp}.md")
        with open(briefing_file, 'w', encoding='utf-8') as f:
            f.write(briefing)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"   ç®€æŠ¥æ–‡ä»¶: {briefing_file}")
        
        return briefing_file
    
    def run(self):
        """è¿è¡Œæ–°é—»çˆ¬å–ç³»ç»Ÿ"""
        print("=" * 60)
        print("ğŸ“° æ–°é—»çˆ¬å–ç³»ç»Ÿå¯åŠ¨")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. çˆ¬å–æ‰€æœ‰æ•°æ®æº
            articles = self.crawl_all_sources()
            
            # 2. ç”Ÿæˆç®€æŠ¥
            briefing = self.generate_briefing(articles)
            
            # 3. ä¿å­˜ç»“æœ
            briefing_file = self.save_results(articles, briefing)
            
            # 4. æ˜¾ç¤ºç®€æŠ¥é¢„è§ˆ
            print("\n" + "=" * 60)
            print("ğŸ“„ ç®€æŠ¥é¢„è§ˆ:")
            print("=" * 60)
            print(briefing[:500] + "..." if len(briefing) > 500 else briefing)
            print("=" * 60)
            
            elapsed_time = time.time() - start_time
            print(f"\nâœ… æ–°é—»çˆ¬å–ç³»ç»Ÿè¿è¡Œå®Œæˆ!")
            print(f"   è€—æ—¶: {elapsed_time:.1f}ç§’")
            print(f"   ç®€æŠ¥æ–‡ä»¶: {briefing_file}")
            
            return briefing_file
            
        except Exception as e:
            print(f"\nâŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    crawler = NewsCrawlerSystem()
    crawler.run()

if __name__ == "__main__":
    main()