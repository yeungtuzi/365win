#!/usr/bin/env python3
# æ¯æ—¥çˆ¬å–è°ƒåº¦å™¨

import os
import sys
import json
from datetime import datetime, timedelta
import schedule
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.full_content_crawler import FullContentCrawler

class DailyCrawlScheduler:
    """æ¯æ—¥çˆ¬å–è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.crawler = FullContentCrawler()
        self.log_file = "logs/crawl_scheduler.log"
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
    
    def log(self, message: str):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        
        print(log_message)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + "\n")
    
    def run_daily_crawl(self):
        """è¿è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡"""
        self.log("å¼€å§‹æ¯æ—¥çˆ¬å–ä»»åŠ¡")
        
        try:
            # è¿è¡Œçˆ¬å–
            result = self.crawler.daily_crawl()
            
            # è®°å½•ç»“æœ
            stats = {
                "date": datetime.now().isoformat(),
                "foreign_articles": len(result["foreign"]),
                "chinese_articles": len(result["chinese"]),
                "total_articles": len(result["foreign"]) + len(result["chinese"]),
                "status": "success"
            }
            
            self.log(f"çˆ¬å–å®Œæˆ: {stats['foreign_articles']}å¤–æ–‡ + {stats['chinese_articles']}ä¸­æ–‡ = {stats['total_articles']}ç¯‡")
            
            # ä¿å­˜ç»Ÿè®¡
            stats_dir = "data/crawl_stats"
            os.makedirs(stats_dir, exist_ok=True)
            stats_file = f"{stats_dir}/stats_{datetime.now().strftime('%Y%m%d')}.json"
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            self.log(f"ç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
            
        except Exception as e:
            error_msg = f"çˆ¬å–ä»»åŠ¡å¤±è´¥: {type(e).__name__}: {e}"
            self.log(error_msg)
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            error_stats = {
                "date": datetime.now().isoformat(),
                "error": str(e),
                "error_type": type(e).__name__,
                "status": "failed"
            }
            
            error_dir = "data/crawl_errors"
            os.makedirs(error_dir, exist_ok=True)
            error_file = f"{error_dir}/error_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
            
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(error_stats, f, ensure_ascii=False, indent=2)
    
    def check_buffer_status(self):
        """æ£€æŸ¥ç¼“å†²çŠ¶æ€"""
        try:
            articles = self.crawler.load_recent_data(days=3)
            
            status = {
                "check_time": datetime.now().isoformat(),
                "total_articles": len(articles),
                "foreign_articles": len([a for a in articles if a["language"] == "en"]),
                "chinese_articles": len([a for a in articles if a["language"] == "zh"]),
                "oldest_article": min([a.get("crawl_date", "") for a in articles], default="æ— æ•°æ®"),
                "newest_article": max([a.get("crawl_date", "") for a in articles], default="æ— æ•°æ®")
            }
            
            self.log(f"ç¼“å†²çŠ¶æ€: {status['total_articles']}ç¯‡æ–‡ç«  ({status['foreign_articles']}å¤–æ–‡, {status['chinese_articles']}ä¸­æ–‡)")
            
            # ä¿å­˜çŠ¶æ€æŠ¥å‘Š
            status_dir = "data/buffer_status"
            os.makedirs(status_dir, exist_ok=True)
            status_file = f"{status_dir}/status_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
            
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, ensure_ascii=False, indent=2)
            
            return status
            
        except Exception as e:
            self.log(f"æ£€æŸ¥ç¼“å†²çŠ¶æ€å¤±è´¥: {e}")
            return None
    
    def setup_schedule(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        # æ¯æ—¥å‡Œæ™¨2ç‚¹è¿è¡Œçˆ¬å–ï¼ˆæœåŠ¡å™¨è´Ÿè½½è¾ƒä½æ—¶ï¼‰
        schedule.every().day.at("02:00").do(self.run_daily_crawl)
        
        # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ç¼“å†²çŠ¶æ€
        schedule.every().hour.do(self.check_buffer_status)
        
        self.log("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®:")
        self.log("  - æ¯æ—¥ 02:00: è¿è¡Œçˆ¬å–ä»»åŠ¡")
        self.log("  - æ¯å°æ—¶: æ£€æŸ¥ç¼“å†²çŠ¶æ€")
    
    def run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        self.log("å¯åŠ¨æ¯æ—¥çˆ¬å–è°ƒåº¦å™¨")
        self.log(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        self.setup_schedule()
        
        # ç«‹å³è¿è¡Œä¸€æ¬¡ç¼“å†²æ£€æŸ¥
        self.check_buffer_status()
        
        # å¦‚æœå½“å‰æ—¶é—´æ¥è¿‘2ç‚¹ï¼Œç«‹å³è¿è¡Œä¸€æ¬¡çˆ¬å–
        current_hour = datetime.now().hour
        if current_hour == 1 or current_hour == 2:  # 1-2ç‚¹ä¹‹é—´
            self.log("å½“å‰æ—¶é—´æ¥è¿‘çˆ¬å–æ—¶é—´ï¼Œç«‹å³è¿è¡Œçˆ¬å–ä»»åŠ¡")
            self.run_daily_crawl()
        
        self.log("è°ƒåº¦å™¨è¿è¡Œä¸­ï¼ŒæŒ‰Ctrl+Cé€€å‡º")
        
        try:
            # ä¸»å¾ªç¯
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
        except KeyboardInterrupt:
            self.log("è°ƒåº¦å™¨å·²åœæ­¢")
        except Exception as e:
            self.log(f"è°ƒåº¦å™¨å¼‚å¸¸é€€å‡º: {e}")

def manual_crawl():
    """æ‰‹åŠ¨è¿è¡Œçˆ¬å–"""
    print("æ‰‹åŠ¨è¿è¡Œæ¯æ—¥çˆ¬å–...")
    
    scheduler = DailyCrawlScheduler()
    scheduler.run_daily_crawl()
    
    # æ£€æŸ¥ç¼“å†²çŠ¶æ€
    status = scheduler.check_buffer_status()
    
    if status:
        print(f"\nç¼“å†²çŠ¶æ€:")
        print(f"  æ€»æ–‡ç« æ•°: {status['total_articles']}")
        print(f"  å¤–æ–‡æ–‡ç« : {status['foreign_articles']}")
        print(f"  ä¸­æ–‡æ–‡ç« : {status['chinese_articles']}")
        print(f"  æœ€æ—©æ–‡ç« : {status['oldest_article'][:19] if status['oldest_article'] != 'æ— æ•°æ®' else 'æ— æ•°æ®'}")
        print(f"  æœ€æ–°æ–‡ç« : {status['newest_article'][:19] if status['newest_article'] != 'æ— æ•°æ®' else 'æ— æ•°æ®'}")

def check_status():
    """æ£€æŸ¥çŠ¶æ€"""
    print("æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
    
    scheduler = DailyCrawlScheduler()
    status = scheduler.check_buffer_status()
    
    if status:
        print(f"\nğŸ“Š ç¼“å†²çŠ¶æ€æŠ¥å‘Š:")
        print(f"  æ€»æ–‡ç« æ•°: {status['total_articles']}ç¯‡")
        print(f"  å¤–æ–‡æ–‡ç« : {status['foreign_articles']}ç¯‡ ({(status['foreign_articles']/max(status['total_articles'],1))*100:.1f}%)")
        print(f"  ä¸­æ–‡æ–‡ç« : {status['chinese_articles']}ç¯‡ ({(status['chinese_articles']/max(status['total_articles'],1))*100:.1f}%)")
        
        if status['total_articles'] > 0:
            print(f"\nğŸ“… æ•°æ®æ—¶é—´èŒƒå›´:")
            print(f"  æœ€æ—©: {status['oldest_article'][:19]}")
            print(f"  æœ€æ–°: {status['newest_article'][:19]}")
            
            # è®¡ç®—æ•°æ®æ–°é²œåº¦
            newest_date = datetime.fromisoformat(status['newest_article'].replace('Z', '+00:00'))
            hours_old = (datetime.now() - newest_date).total_seconds() / 3600
            
            if hours_old < 24:
                print(f"  æ•°æ®æ–°é²œåº¦: âœ… {hours_old:.1f}å°æ—¶å‰")
            elif hours_old < 72:
                print(f"  æ•°æ®æ–°é²œåº¦: âš ï¸ {hours_old/24:.1f}å¤©å‰")
            else:
                print(f"  æ•°æ®æ–°é²œåº¦: âŒ {hours_old/24:.1f}å¤©å‰ï¼ˆå»ºè®®è¿è¡Œçˆ¬å–ï¼‰")
        else:
            print("\nâŒ ç¼“å†²ä¸­æ²¡æœ‰æ•°æ®ï¼Œè¯·è¿è¡Œçˆ¬å–ä»»åŠ¡")
    else:
        print("âŒ æ— æ³•è·å–çŠ¶æ€ä¿¡æ¯")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¯æ—¥çˆ¬å–è°ƒåº¦å™¨")
    parser.add_argument("--crawl", action="store_true", help="æ‰‹åŠ¨è¿è¡Œçˆ¬å–")
    parser.add_argument("--status", action="store_true", help="æ£€æŸ¥çŠ¶æ€")
    parser.add_argument("--start", action="store_true", help="å¯åŠ¨è°ƒåº¦å™¨")
    
    args = parser.parse_args()
    
    if args.crawl:
        manual_crawl()
    elif args.status:
        check_status()
    elif args.start:
        scheduler = DailyCrawlScheduler()
        scheduler.run_scheduler()
    else:
        print("è¯·æŒ‡å®šæ“ä½œ:")
        print("  --crawl   æ‰‹åŠ¨è¿è¡Œçˆ¬å–")
        print("  --status  æ£€æŸ¥çŠ¶æ€")
        print("  --start   å¯åŠ¨è°ƒåº¦å™¨")