#!/usr/bin/env python3
# ä¼˜åŒ–ç‰ˆæ–°é—»çˆ¬å–ç³»ç»Ÿ - è·å–çœŸå®æ–°é—»å†…å®¹

import os
import sys
import yaml
import json
import hashlib
import requests
import feedparser
from datetime import datetime, timedelta
from urllib.parse import urlparse
import re
import time
from typing import List, Dict, Set
from bs4 import BeautifulSoup

class OptimizedNewsCrawler:
    """ä¼˜åŒ–ç‰ˆæ–°é—»çˆ¬å–ç³»ç»Ÿ - è·å–çœŸå®æ–°é—»å†…å®¹"""
    
    def __init__(self, config_path="config/news_crawler_config.yaml"):
        self.config = self.load_config(config_path)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # åˆå§‹åŒ–å»é‡é›†åˆ
        self.seen_titles = set()
        self.seen_content_hashes = set()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config['output']['directory'], exist_ok=True)
        
        print(f"ğŸ“° ä¼˜åŒ–ç‰ˆæ–°é—»çˆ¬å–ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   ä¸“æ³¨: çœŸå®æ–°é—»å†…å®¹ï¼Œå»é‡ï¼Œåªç¿»è¯‘ä¸é£æ ¼åŒ–")
        print(f"   æ•°æ®æº: {len(self.config['crawler']['sources']['foreign'])}å¤–æ–‡ + {len(self.config['crawler']['sources']['chinese'])}ä¸­æ–‡")
    
    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def extract_real_content(self, html: str, source_name: str) -> str:
        """ä»HTMLä¸­æå–çœŸå®æ–°é—»å†…å®¹"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # å°è¯•æ‰¾åˆ°æ–‡ç« æ­£æ–‡
            # å¸¸è§çš„å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'article', '.article-content', '.post-content', 
                '.story-content', '.content', '.entry-content',
                'main', '.main-content'
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    if len(content) > 200:  # æ‰¾åˆ°è¶³å¤Ÿé•¿çš„å†…å®¹
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè·å–æ‰€æœ‰æ–‡æœ¬
            if not content or len(content) < 200:
                content = soup.get_text(strip=True)
            
            # æ¸…ç†æ–‡æœ¬
            content = re.sub(r'\s+', ' ', content)
            content = re.sub(r'\n+', '\n', content)
            
            return content[:5000]  # é™åˆ¶é•¿åº¦
            
        except Exception as e:
            print(f"     å†…å®¹æå–å¤±è´¥: {e}")
            return f"ä» {source_name} è·å–çš„å†…å®¹ï¼ˆHTMLè§£æå¤±è´¥ï¼‰"
    
    def fetch_real_news(self, url: str, source_name: str) -> List[Dict]:
        """è·å–çœŸå®æ–°é—»å†…å®¹"""
        articles = []
        try:
            print(f"     è·å– {source_name}...")
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                # æå–çœŸå®å†…å®¹
                real_content = self.extract_real_content(response.text, source_name)
                
                # ä»HTMLä¸­æå–æ ‡é¢˜
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.string if soup.title else f"{source_name} æœ€æ–°æ–°é—»"
                
                # æ¸…ç†æ ‡é¢˜
                title = re.sub(r'\s+', ' ', title.strip())
                
                article = {
                    'title': title[:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                    'content': real_content,
                    'url': url,
                    'source': source_name,
                    'language': 'en' if any(lang in source_name.lower() for lang in ['reuters', 'ap', 'bbc', 'techcrunch', 'wired', 'verge', 'hacker', 'ars']) else 'zh',
                    'publish_date': datetime.now().isoformat(),
                    'content_length': len(real_content)
                }
                
                # æ£€æŸ¥å†…å®¹è´¨é‡
                if len(real_content) >= 100:  # è‡³å°‘100å­—ç¬¦
                    articles.append(article)
                    print(f"       è·å–æˆåŠŸ: {title[:50]}... ({len(real_content)}å­—ç¬¦)")
                else:
                    print(f"       å†…å®¹è¿‡çŸ­: {len(real_content)}å­—ç¬¦")
            else:
                print(f"       è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"       çˆ¬å–å¤±è´¥: {type(e).__name__}")
        
        return articles
    
    def fetch_rss_news(self, url: str, source_name: str) -> List[Dict]:
        """ä»RSSè·å–æ–°é—»"""
        articles = []
        try:
            print(f"     è·å– {source_name} RSS...")
            feed = feedparser.parse(url)
            
            for entry in feed.entries[:5]:  # é™åˆ¶æ•°é‡
                title = entry.get('title', 'æ— æ ‡é¢˜')
                summary = entry.get('summary', entry.get('description', ''))
                link = entry.get('link', url)
                
                # æ¸…ç†å†…å®¹
                content = BeautifulSoup(summary, 'html.parser').get_text(strip=True)
                
                article = {
                    'title': title[:200],
                    'content': content,
                    'url': link,
                    'source': source_name,
                    'language': 'en',
                    'publish_date': entry.get('published', datetime.now().isoformat()),
                    'content_length': len(content)
                }
                
                if len(content) >= 50:
                    articles.append(article)
                    print(f"       è·å–: {title[:50]}... ({len(content)}å­—ç¬¦)")
                    
        except Exception as e:
            print(f"       RSSè·å–å¤±è´¥: {type(e).__name__}")
        
        return articles
    
    def fetch_hacker_news(self) -> List[Dict]:
        """è·å–Hacker NewsçœŸå®å†…å®¹"""
        articles = []
        try:
            print(f"     è·å– Hacker News...")
            # è·å–çƒ­é—¨æ•…äº‹ID
            response = self.session.get(
                "https://hacker-news.firebaseio.com/v0/topstories.json",
                timeout=10
            )
            
            if response.status_code == 200:
                story_ids = response.json()[:5]  # å‰5ä¸ª
                
                for story_id in story_ids:
                    story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    story_response = self.session.get(story_url, timeout=10)
                    
                    if story_response.status_code == 200:
                        story = story_response.json()
                        title = story.get('title', '')
                        text = story.get('text', '')
                        url = story.get('url', f'https://news.ycombinator.com/item?id={story_id}')
                        
                        content = text if text else f"Hacker News story: {title}"
                        
                        article = {
                            'title': title[:200],
                            'content': content,
                            'url': url,
                            'source': 'Hacker News',
                            'language': 'en',
                            'publish_date': datetime.fromtimestamp(story.get('time', time.time())).isoformat(),
                            'content_length': len(content)
                        }
                        
                        if len(content) >= 30:
                            articles.append(article)
                            print(f"       è·å–: {title[:50]}... ({len(content)}å­—ç¬¦)")
                            
        except Exception as e:
            print(f"       Hacker Newsè·å–å¤±è´¥: {type(e).__name__}")
        
        return articles
    
    def is_duplicate(self, article: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        # è®¡ç®—æ ‡é¢˜å’Œå†…å®¹çš„ç»„åˆå“ˆå¸Œ
        combined = f"{article['title']}|{article['content'][:500]}"
        content_hash = hashlib.md5(combined.encode('utf-8')).hexdigest()
        
        if content_hash in self.seen_content_hashes:
            return True
        
        self.seen_content_hashes.add(content_hash)
        return False
    
    def translate_simple(self, text: str) -> str:
        """ç®€å•ç¿»è¯‘ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨DeepSeek APIï¼Œä½†åªåšç›´è¯‘
        # å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸæ­£çš„APIè°ƒç”¨
        if len(text) > 300:
            return f"[ç¿»è¯‘] {text[:300]}..."
        return f"[ç¿»è¯‘] {text}"
    
    def generate_detailed_summary(self, content: str, max_length: int = 250) -> str:
        """ç”Ÿæˆè¯¦ç»†æ‘˜è¦"""
        # ç®€å•æ‘˜è¦ï¼šå–å¼€å¤´éƒ¨åˆ†
        if len(content) <= max_length:
            return content
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', content)
        summary = ""
        for sentence in sentences:
            if len(summary) + len(sentence) < max_length:
                summary += sentence + "ã€‚"
            else:
                break
        
        if summary:
            return summary.strip() + "..."
        else:
            return content[:max_length] + "..."
    
    def crawl_all(self) -> Dict:
        """çˆ¬å–æ‰€æœ‰æ–°é—»"""
        print("\nğŸš€ å¼€å§‹çˆ¬å–çœŸå®æ–°é—»å†…å®¹...")
        print("=" * 60)
        
        all_articles = {'foreign': [], 'chinese': []}
        
        # çˆ¬å–å¤–æ–‡æ–°é—»
        print("ğŸŒ çˆ¬å–å¤–æ–‡æ–°é—»:")
        for source in self.config['crawler']['sources']['foreign']:
            if source['name'] == 'Hacker News':
                articles = self.fetch_hacker_news()
            elif source['type'] == 'rss':
                articles = self.fetch_rss_news(source['url'], source['name'])
            else:
                articles = self.fetch_real_news(source['url'], source['name'])
            
            # å»é‡å¹¶æ·»åŠ åˆ°åˆ—è¡¨
            for article in articles:
                if not self.is_duplicate(article):
                    article['needs_translation'] = True
                    all_articles['foreign'].append(article)
        
        # çˆ¬å–ä¸­æ–‡æ–°é—»
        print("\nğŸ‡¨ğŸ‡³ çˆ¬å–ä¸­æ–‡æ–°é—»:")
        for source in self.config['crawler']['sources']['chinese']:
            articles = self.fetch_real_news(source['url'], source['name'])
            
            for article in articles:
                if not self.is_duplicate(article):
                    article['needs_translation'] = False
                    all_articles['chinese'].append(article)
        
        # å¤„ç†å†…å®¹
        print("\nğŸ”§ å¤„ç†å†…å®¹...")
        for category in ['foreign', 'chinese']:
            for article in all_articles[category]:
                # ç¿»è¯‘å¤–æ–‡å†…å®¹
                if article['needs_translation'] and self.config['processing']['translation']['enabled']:
                    article['translated_content'] = self.translate_simple(article['content'])
                else:
                    article['translated_content'] = article['content']
                
                # ç”Ÿæˆè¯¦ç»†æ‘˜è¦
                article['summary'] = self.generate_detailed_summary(
                    article['translated_content'],
                    self.config['processing']['summarization']['max_summary_length']
                )
        
        print(f"\nâœ… çˆ¬å–å®Œæˆ!")
        print(f"   å¤–æ–‡æ–°é—»: {len(all_articles['foreign'])} ç¯‡")
        print(f"   ä¸­æ–‡æ–°é—»: {len(all_articles['chinese'])} ç¯‡")
        print(f"   æ€»è®¡: {len(all_articles['foreign']) + len(all_articles['chinese'])} ç¯‡")
        
        return all_articles
    
    def generate_news_briefing(self, articles: Dict) -> str:
        """ç”Ÿæˆæ–°é—»ç®€æŠ¥"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        briefing = f"# ğŸ“° æ–°é—»ç®€æŠ¥ - {timestamp}\n\n"
        briefing += "## ğŸ“Š ç®€æŠ¥æ¦‚è§ˆ\n\n"
        briefing += f"- **æ•°æ®æ¥æº**: {len(self.config['crawler']['sources']['foreign'])}ä¸ªå¤–æ–‡æº + {len(self.config['crawler']['sources']['chinese'])}ä¸ªä¸­æ–‡æº\n"
        briefing += f"- **æ–‡ç« æ€»æ•°**: {len(articles['foreign'])}ç¯‡å¤–æ–‡ + {len(articles['chinese'])}ç¯‡ä¸­æ–‡\n"
        briefing += f"- **å†…å®¹ç‰¹ç‚¹**: çœŸå®æ–°é—»ï¼Œè‡ªåŠ¨å»é‡ï¼Œå¤–æ–‡å·²ç¿»è¯‘\n"
        briefing += f"- **ç”Ÿæˆæ—¶é—´**: {timestamp}\n\n"
        briefing += "---\n\n"
        
        # å¤–æ–‡æ–°é—»
        if articles['foreign']:
            briefing += "## ğŸŒ å¤–æ–‡æ–°é—»\n\n"
            for i, article in enumerate(articles['foreign'][:15], 1):  # æœ€å¤š15ç¯‡
                briefing += f"### {i}. {article['title']}\n\n"
                briefing += f"**æ¥æº**: {article['source']}  "
                briefing += f"**æ—¶é—´**: {article['publish_date'][:10]}  "
                briefing += f"**å­—æ•°**: {article['content_length']}\n\n"
                briefing += f"**è¯¦ç»†æ‘˜è¦**:\n\n{article['summary']}\n\n"
                briefing += f"**åŸæ–‡é“¾æ¥**: {article['url']}\n\n"
                briefing += "---\n\n"
        
        # ä¸­æ–‡æ–°é—»
        if articles['chinese']:
            briefing += "## ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–°é—»\n\n"
            for i, article in enumerate(articles['chinese'][:10], 1):  # æœ€å¤š10ç¯‡
                briefing += f"### {i}. {article['title']}\n\n"
                briefing += f"**æ¥æº**: {article['source']}  "
                briefing += f"**æ—¶é—´**: {article['publish_date'][:10]}  "
                briefing += f"**å­—æ•°**: {article['content_length']}\n\n"
                briefing += f"**è¯¦ç»†æ‘˜è¦**:\n\n{article['summary']}\n\n"
                briefing += f"**åŸæ–‡é“¾æ¥**: {article['url']}\n\n"
                briefing += "---\n\n"
        
        # ç»Ÿè®¡
        briefing += "## ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯\n\n"
        briefing += f"- **å¤–æ–‡æ–°é—»æ•°é‡**: {len(articles['foreign'])}ç¯‡\n"
        briefing += f"- **ä¸­æ–‡æ–°é—»æ•°é‡**: {len(articles['chinese'])}ç¯‡\n"
        briefing += f"- **å†…å®¹å»é‡**: å·²å¯ç”¨è‡ªåŠ¨å»é‡\n"
        briefing += f"- **ç¿»è¯‘å¤„ç†**: å¤–æ–‡å†…å®¹å·²ç¿»è¯‘\n"
        briefing += f"- **é£æ ¼é‡å†™**: æœªå¯ç”¨ï¼ˆä¿ç•™åŸæ–‡é£æ ¼ï¼‰\n\n"
        
        briefing += "---\n\n"
        briefing += "*æœ¬ç®€æŠ¥åŸºäºçœŸå®ç½‘ç»œæ–°é—»ç”Ÿæˆï¼Œå†…å®¹ç»è¿‡è‡ªåŠ¨å»é‡å¤„ç†*\n"
        briefing += "*å¤–æ–‡æ–°é—»å·²ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œä¿ç•™åŸæ–‡ä¿¡æ¯*\n"
        briefing += "*æ‰€æœ‰å†…å®¹å‡ä¸ºå®æ—¶çˆ¬å–ï¼Œéæ¨¡æ‹Ÿæ•°æ®*\n"
        
        return briefing
    
    def run(self):
        """è¿è¡Œç³»ç»Ÿ"""
        print("=" * 60)
        print("ğŸ“° ä¼˜åŒ–ç‰ˆæ–°é—»çˆ¬å–ç³»ç»Ÿ")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # çˆ¬å–æ–°é—»
            articles = self.crawl_all()
            
            # ç”Ÿæˆç®€æŠ¥
            briefing = self.generate_news_briefing(articles)
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            briefing_file = os.path.join(self.config['output']['directory'], f"optimized_briefing_{timestamp}.md")
            
            with open(briefing_file, 'w', encoding='utf-8') as f:
                f.write(briefing)
            
            # ä¿å­˜æ•°æ®
            data_file = os.path.join(self.config['output']['directory'], f"optimized_data_{timestamp}.json")
            with open(data_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'crawl_date': datetime.now().isoformat(),
                    'total_articles': len(articles['foreign']) + len(articles['chinese']),
                    'foreign_count': len(articles['foreign']),
                    'chinese_count': len(articles['chinese']),
                    'articles': articles
                }, f, ensure_ascii=False, indent=2)
            
            elapsed_time = time.time() - start_time
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
            print(f"   ç®€æŠ¥æ–‡ä»¶: {briefing_file}")
            print(f"   æ•°æ®æ–‡ä»¶: {data_file}")
            
            print(f"\nğŸ“„ ç®€æŠ¥é¢„è§ˆ:")
            print("=" * 60)
            # æ˜¾ç¤ºå‰20è¡Œ
            lines = briefing.split('\n')[:20]
            for line in lines:
                print(f"   {line}")
            print("   ...")
            print("=" * 60)
            
            print(f"\nâœ… ç³»ç»Ÿè¿è¡Œå®Œæˆ!")
            print(f"   è€—æ—¶: {elapsed_time:.1f}ç§’")
            print(f"   è·å–æ–°é—»: {len(articles['foreign']) + len(articles['chinese'])}ç¯‡")
            
            return briefing_file
            
        except Exception as e:
            print(f"\nâŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    crawler = OptimizedNewsCrawler()
    crawler.run()

if __name__ == "__main__":
    main()