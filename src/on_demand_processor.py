#!/usr/bin/env python3
# æŒ‰éœ€å¤„ç†å¼•æ“

import os
import sys
import json
from datetime import datetime
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.full_content_crawler import FullContentCrawler
from scripts.deepseek_client import DeepSeekClient
from scripts.content_processor import ContentProcessor
from scripts.recommendation_engine import RecommendationEngine

class OnDemandProcessor:
    """æŒ‰éœ€å¤„ç†å¼•æ“"""
    
    def __init__(self, api_key: str = None):
        # åˆå§‹åŒ–ç»„ä»¶
        self.crawler = FullContentCrawler()
        
        # DeepSeekå®¢æˆ·ç«¯
        if not api_key:
            api_key = os.getenv("DEEPSEEK_API_KEY")
        
        if api_key and api_key != "test_mode_key":
            self.deepseek = DeepSeekClient(api_key)
            self.test_mode = False
            print("âœ… ä½¿ç”¨çœŸå®çš„DeepSeek API")
        else:
            # åˆ›å»ºæ¨¡æ‹Ÿå®¢æˆ·ç«¯
            self.deepseek = self.create_mock_client()
            self.test_mode = True
            print("âš ï¸ ä½¿ç”¨æ¨¡æ‹ŸDeepSeekå®¢æˆ·ç«¯ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        
        # å†…å®¹å¤„ç†å™¨
        self.processor = ContentProcessor(self.deepseek, "config/system_config.yaml")
        
        # æ¨èå¼•æ“
        self.recommender = RecommendationEngine("config/user_profile.json", self.deepseek)
        
        # è¾“å‡ºç›®å½•
        self.output_dir = "data/processed_output"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print("âœ… æŒ‰éœ€å¤„ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def create_mock_client(self):
        """åˆ›å»ºæ¨¡æ‹Ÿå®¢æˆ·ç«¯"""
        class MockDeepSeekClient:
            def __init__(self):
                self.request_count = 0
            
            def analyze_content(self, text):
                self.request_count += 1
                return {
                    "sentiment_score": 0.7,
                    "patriotic_level": 0.6,
                    "tech_relevance": 0.5,
                    "formality": 0.6,
                    "sensationalism": 0.3,
                    "clickbait_score": 0.2,
                    "main_topics": ["æµ‹è¯•"],
                    "recommended_action": "keep"
                }
            
            def rewrite_content(self, text, style_requirements):
                self.request_count += 1
                return f"ã€çˆ±å›½é”®ç›˜ä¾ é£æ ¼é‡å†™ã€‘{text[:200]}..."
            
            def translate_content(self, text, target_lang="zh"):
                self.request_count += 1
                return f"ã€ç¿»è¯‘ã€‘{text}"
            
            def generate_briefing(self, content_items, briefing_type):
                self.request_count += 1
                return f"ã€{briefing_type}ç®€æŠ¥ã€‘æµ‹è¯•ç®€æŠ¥å†…å®¹"
        
        return MockDeepSeekClient()
    
    def load_content_for_processing(self, use_cached: bool = True) -> List[Dict]:
        """åŠ è½½å¾…å¤„ç†çš„å†…å®¹"""
        print(f"åŠ è½½å†…å®¹ï¼ˆä½¿ç”¨ç¼“å­˜: {use_cached}ï¼‰...")
        
        articles = self.crawler.get_content_for_processing(use_cached=use_cached)
        
        if not articles:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„å†…å®¹")
            return []
        
        print(f"âœ… åŠ è½½ {len(articles)} ç¯‡æ–‡ç« ")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        formatted_items = []
        for i, article in enumerate(articles):
            formatted_item = {
                "id": f"article_{i:03d}",
                "title": article.get("title", "æ— æ ‡é¢˜"),
                "content": article.get("content", ""),
                "source": article.get("source", "æœªçŸ¥æ¥æº"),
                "url": article.get("url", ""),
                "publish_time": article.get("crawl_time", datetime.now().isoformat()),
                "type": article.get("category", "general"),
                "needs_translation": article.get("needs_translation", False),
                "original_language": article.get("original_language", "en"),
                "raw_data": article
            }
            formatted_items.append(formatted_item)
        
        return formatted_items
    
    def process_content(self, items: List[Dict]) -> List[Dict]:
        """å¤„ç†å†…å®¹"""
        print(f"å¼€å§‹å¤„ç† {len(items)} ç¯‡æ–‡ç« ...")
        
        processed_items = []
        
        for i, item in enumerate(items):
            print(f"  å¤„ç† {i+1}/{len(items)}: {item['title'][:40]}...")
            
            # 1. ç¿»è¯‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if item.get("needs_translation"):
                print(f"    ç¿»è¯‘å¤–æ–‡å†…å®¹...")
                translated = self.deepseek.translate_content(item["content"], target_lang="zh")
                item["translated_content"] = translated
                item["content"] = translated  # ä½¿ç”¨ç¿»è¯‘åçš„å†…å®¹è¿›è¡Œåç»­å¤„ç†
            
            # 2. å†…å®¹åˆ†æ
            print(f"    åˆ†æå†…å®¹...")
            analysis = self.deepseek.analyze_content(item["content"])
            item["analysis"] = analysis
            
            # 3. å†…å®¹é‡å†™ï¼ˆè½¬ä¸ºçˆ±å›½é”®ç›˜ä¾ é£æ ¼ï¼‰
            print(f"    é‡å†™å†…å®¹...")
            style_requirements = {
                "ç›®æ ‡é£æ ¼": "çˆ±å›½é”®ç›˜ä¾ åå¥½",
                "è¦æ±‚": "ç†æ€§å†·é™ã€ç”¨è¯ç²¾å‡†ã€é€»è¾‘æ¸…æ™°ã€å¢å¼ºçˆ±å›½æƒ…æ€€",
                "é¿å…": "å°æ¸…æ–°ã€è½»ä½»è¯­æ°”ã€é˜´è°‹è®ºã€è´Ÿé¢æƒ…ç»ª"
            }
            rewritten = self.deepseek.rewrite_content(item["content"], style_requirements)
            item["rewritten_content"] = rewritten
            
            # 4. æƒ…æ„Ÿå¢å¼º
            if analysis.get("sentiment_score", 0) < 0.6:
                print(f"    å¢å¼ºæƒ…æ„Ÿ...")
                enhancement_prompt = f"è¯·å¢å¼ºä»¥ä¸‹å†…å®¹çš„çˆ±å›½æƒ…æ€€å’Œæ­£é¢æƒ…æ„Ÿ:\n\n{rewritten}"
                enhanced = self.deepseek.call_api(
                    enhancement_prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªçˆ±å›½æƒ…æ„Ÿå¢å¼ºä¸“å®¶",
                    temperature=0.3,
                    max_tokens=1000
                )
                item["enhanced_content"] = enhanced
            else:
                item["enhanced_content"] = rewritten
            
            processed_items.append(item)
            
            # è¿›åº¦æ˜¾ç¤º
            if (i + 1) % 3 == 0 or i == len(items) - 1:
                print(f"    è¿›åº¦: {i+1}/{len(items)} å®Œæˆ")
        
        print(f"âœ… å¤„ç†å®Œæˆ: {len(processed_items)} ç¯‡æ–‡ç« ")
        return processed_items
    
    def generate_recommendations(self, items: List[Dict], count: int = 3) -> List[Dict]:
        """ç”Ÿæˆæ¨è"""
        print(f"ç”Ÿæˆæ¨èï¼ˆé€‰æ‹© {count} ç¯‡ï¼‰...")
        
        # ä½¿ç”¨æ¨èå¼•æ“
        recommendations = self.recommender.recommend_content(items, count=count)
        
        print(f"âœ… æ¨è {len(recommendations)} ç¯‡æ–‡ç« ")
        return recommendations
    
    def generate_briefing(self, recommendations: List[Dict], briefing_type: str = "daily") -> str:
        """ç”Ÿæˆç®€æŠ¥"""
        print(f"ç”Ÿæˆ{briefing_type}ç®€æŠ¥...")
        
        # å‡†å¤‡ç®€æŠ¥å†…å®¹
        briefing_items = []
        for i, rec in enumerate(recommendations):
            content = rec.get("enhanced_content", rec.get("rewritten_content", rec.get("content", "")))
            briefing_items.append({
                "title": rec["title"],
                "content": content[:500],  # é™åˆ¶é•¿åº¦
                "source": rec["source"],
                "score": rec.get("recommendation_score", 0.5)
            })
        
        # ç”Ÿæˆç®€æŠ¥
        briefing = self.deepseek.generate_briefing(briefing_items, briefing_type)
        
        if not briefing:
            # å¤‡ç”¨æ–¹æ¡ˆ
            briefing = self.generate_fallback_briefing(recommendations, briefing_type)
        
        print(f"âœ… ç®€æŠ¥ç”Ÿæˆå®Œæˆ ({len(briefing)} å­—ç¬¦)")
        return briefing
    
    def generate_fallback_briefing(self, recommendations: List[Dict], briefing_type: str) -> str:
        """ç”Ÿæˆå¤‡ç”¨ç®€æŠ¥"""
        type_names = {
            "daily": "æ¯æ—¥ç²¾é€‰",
            "morning": "æ—©å®‰ç®€æŠ¥",
            "noon": "åˆé—´ç²¾é€‰",
            "evening": "æ™šé—´å›é¡¾"
        }
        
        title = type_names.get(briefing_type, "ç²¾é€‰ç®€æŠ¥")
        
        briefing = f"ã€ä¸€å¹´365èµ¢ã€‘{title}\n"
        briefing += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
        briefing += "=" * 50 + "\n\n"
        
        for i, rec in enumerate(recommendations):
            briefing += f"{i+1}. **{rec['title']}**\n"
            briefing += f"   æ¥æº: {rec['source']}\n"
            
            content = rec.get("enhanced_content", rec.get("rewritten_content", rec.get("content", "")))
            summary = content[:200] + "..." if len(content) > 200 else content
            briefing += f"   æ‘˜è¦: {summary}\n\n"
        
        briefing += "=" * 50 + "\n"
        briefing += "â¤ï¸ å–œæ¬¢ | ğŸ‘ ä¸å–œæ¬¢ | ğŸ”„ æ¢ä¸€æ‰¹\n"
        briefing += "#çˆ±å›½ #æ­£èƒ½é‡ #ä¸€å¹´365èµ¢"
        
        return briefing
    
    def save_output(self, recommendations: List[Dict], briefing: str, output_type: str = "daily"):
        """ä¿å­˜è¾“å‡º"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ä¿å­˜æ¨èå†…å®¹
        rec_file = f"{self.output_dir}/recommendations_{timestamp}.json"
        with open(rec_file, 'w', encoding='utf-8') as f:
            json.dump(recommendations, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜ç®€æŠ¥
        brief_file = f"{self.output_dir}/briefing_{timestamp}.txt"
        with open(brief_file, 'w', encoding='utf-8') as f:
            f.write(briefing)
        
        # 3. ä¿å­˜å¤„ç†ç»Ÿè®¡
        stats = {
            "timestamp": datetime.now().isoformat(),
            "output_type": output_type,
            "recommendations_count": len(recommendations),
            "briefing_length": len(briefing),
            "deepseek_requests": self.deepseek.request_count if hasattr(self.deepseek, 'request_count') else 0,
            "test_mode": self.test_mode
        }
        
        stats_file = f"{self.output_dir}/stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¾“å‡ºå·²ä¿å­˜:")
        print(f"   æ¨èå†…å®¹: {rec_file}")
        print(f"   ç®€æŠ¥: {brief_file}")
        print(f"   ç»Ÿè®¡: {stats_file}")
        
        return {
            "recommendations_file": rec_file,
            "briefing_file": brief_file,
            "stats_file": stats_file
        }
    
    def run_on_demand(self, output_type: str = "daily", use_cached: bool = True):
        """è¿è¡ŒæŒ‰éœ€å¤„ç†"""
        print(f"ğŸš€ å¼€å§‹æŒ‰éœ€å¤„ç†: {output_type}")
        print("=" * 60)
        
        start_time = datetime.now()
        
        try:
            # 1. åŠ è½½å†…å®¹
            items = self.load_content_for_processing(use_cached=use_cached)
            
            if not items:
                print("âŒ æ²¡æœ‰å¯å¤„ç†çš„å†…å®¹ï¼Œè¯·å…ˆè¿è¡Œçˆ¬å–ä»»åŠ¡")
                return None
            
            # 2. å¤„ç†å†…å®¹
            processed_items = self.process_content(items)
            
            # 3. ç”Ÿæˆæ¨è
            recommendations = self.generate_recommendations(processed_items, count=3)
            
            # 4. ç”Ÿæˆç®€æŠ¥
            briefing = self.generate_briefing(recommendations, output_type)
            
            # 5. ä¿å­˜è¾“å‡º
            output_files = self.save_output(recommendations, briefing, output_type)
            
            # 6. æ˜¾ç¤ºç®€æŠ¥
            print("\n" + "=" * 60)
            print("ğŸ“¨ ç”Ÿæˆçš„ç®€æŠ¥:")
            print("=" * 60)
            print(briefing[:1000] + "..." if len(briefing) > 1000 else briefing)
            print("=" * 60)
            
            # 7. ç»Ÿè®¡ä¿¡æ¯
            duration = (datetime.now() - start_time).total_seconds()
            print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   å¤„ç†æ—¶é—´: {duration:.1f}ç§’")
            print(f"   å¤„ç†æ–‡ç« : {len(processed_items)}ç¯‡")
            print(f"   æ¨èæ–‡ç« : {len(recommendations)}ç¯‡")
            print(f"   DeepSeekè¯·æ±‚: {self.deepseek.request_count if hasattr(self.deepseek, 'request_count') else 'N/A'}æ¬¡")
            print(f"   è¾“å‡ºç±»å‹: {output_type}")
            
            return {
                "success": True,
                "briefing": briefing,
                "recommendations": recommendations,
                "output_files": output_files,
                "duration": duration,
                "processed_count": len(processed_items)
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æŒ‰éœ€å¤„ç†å¼•æ“")
    parser.add_argument("--type", choices=["daily", "morning", "noon", "evening"], 
                       default="daily", help="è¾“å‡ºç±»å‹")
    parser.add_argument("--refresh", action="store_true", 
                       help="å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰")
    parser.add_argument("--api-key", help="DeepSeek APIå¯†é’¥")
    
    args = parser.parse_args()
    
    # è®¾ç½®APIå¯†é’¥
    if args.api_key:
        os.environ["DEEPSEEK_API_KEY"] = args.api_key
    
    print("ğŸ¯ ä¸€å¹´365èµ¢ - æŒ‰éœ€å¤„ç†å¼•æ“")
    print("=" * 60)
    
    # è¿è¡Œå¤„ç†
    processor = OnDemandProcessor()
    result = processor.run_on_demand(
        output_type=args.type,
        use_cached=not args.refresh
    )
    
    if result and result.get("success"):
        print("\n" + "=" * 60)
        print("ğŸ‰ æŒ‰éœ€å¤„ç†å®Œæˆï¼")
        print(f"âœ¨ å·²ç”Ÿæˆçˆ±å›½é”®ç›˜ä¾ é£æ ¼çš„{args.type}ç®€æŠ¥")
        print("ğŸ‡¨ğŸ‡³ ä¸€å¹´365èµ¢ï¼Œå¤©å¤©éƒ½åœ¨èµ¢ï¼")
        print("=" * 60)
    else:
        print("\nâŒ å¤„ç†å¤±è´¥")

if __name__ == "__main__":
    main()