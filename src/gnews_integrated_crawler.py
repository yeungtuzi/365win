#!/usr/bin/env python3
# gnews.ioé›†æˆç‰ˆæ–°é—»çˆ¬å–ç³»ç»Ÿ

import os
import sys
import json
import requests
import hashlib
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Set
import yaml

class GNewsIntegratedCrawler:
    """é›†æˆgnews.ioçš„æ–°é—»çˆ¬å–ç³»ç»Ÿ"""
    
    def __init__(self, config_path="config/news_crawler_config.yaml"):
        self.config = self.load_config(config_path)
        
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        self.gnews_api_key = os.getenv("GNEWS_API_KEY", "")
        self.gnews_base_url = "https://gnews.io/api/v4"
        
        # DeepSeek APIé…ç½®
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY", "")
        self.deepseek_base_url = "https://api.deepseek.com"
        
        # è¯·æ±‚è®¡æ•°å™¨ï¼ˆæ§åˆ¶APIä½¿ç”¨ï¼‰
        self.request_count = 0
        self.max_daily_requests = 60  # å®‰å…¨é™åˆ¶
        
        # å»é‡é›†åˆ
        self.seen_articles = set()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("data/gnews_briefings", exist_ok=True)
        
        # éªŒè¯APIå¯†é’¥
        self.validate_api_keys()
        
        print(f"ğŸ“° gnews.ioé›†æˆç‰ˆæ–°é—»çˆ¬å–ç³»ç»Ÿ")
        print(f"   APIé™åˆ¶: â‰¤{self.max_daily_requests}æ¬¡/å¤©")
        print(f"   ç›®æ ‡: æ¯æ¬¡10-15ç¯‡æ–°é—»ï¼Œå¤–æ–‡ç”¨DeepSeekç¿»è¯‘")
        print(f"   è¾“å‡º: éšè—åŸæ–‡é“¾æ¥ï¼Œä¸“æ³¨å†…å®¹")
    
    def validate_api_keys(self):
        """éªŒè¯APIå¯†é’¥æ˜¯å¦é…ç½®"""
        if not self.gnews_api_key:
            print("âš ï¸  è­¦å‘Š: GNEWS_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
            print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export GNEWS_API_KEY=your_gnews_api_key")
        
        if not self.deepseek_api_key:
            print("âš ï¸  è­¦å‘Š: DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
            print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY=your_deepseek_api_key")
        
        if not self.gnews_api_key or not self.deepseek_api_key:
            print("ğŸ’¡ æç¤º: å¤åˆ¶.env.exampleä¸º.envå¹¶å¡«å…¥APIå¯†é’¥")
    
    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def check_api_limit(self) -> bool:
        """æ£€æŸ¥APIä½¿ç”¨é™åˆ¶"""
        if self.request_count >= self.max_daily_requests:
            print(f"âš ï¸  APIä½¿ç”¨å·²è¾¾ä¸Šé™: {self.request_count}/{self.max_daily_requests}")
            return False
        return True
    
    def call_gnews_api(self, endpoint: str, params: Dict) -> Dict:
        """è°ƒç”¨gnews.io API"""
        if not self.check_api_limit():
            return {'articles': []}
        
        params['token'] = self.gnews_api_key
        params['max'] = 10  # æ¯æ¬¡æœ€å¤š10ç¯‡
        
        try:
            self.request_count += 1
            print(f"   ğŸ“¡ è°ƒç”¨gnews.io API ({self.request_count}/{self.max_daily_requests})...")
            
            response = requests.get(
                f"{self.gnews_base_url}/{endpoint}",
                params=params,
                timeout=15
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"   âŒ gnews.io APIå¤±è´¥: HTTP {response.status_code}")
                return {'articles': []}
                
        except Exception as e:
            print(f"   âŒ APIè°ƒç”¨å¼‚å¸¸: {type(e).__name__}")
            return {'articles': []}
    
    def translate_with_deepseek(self, text: str, source_lang: str = "en") -> str:
        """ä½¿ç”¨DeepSeek APIç¿»è¯‘å†…å®¹"""
        if not text or len(text.strip()) < 10:
            return text
        
        # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨DeepSeek API
        # è¿™é‡Œæ¨¡æ‹Ÿç¿»è¯‘è¿‡ç¨‹
        print(f"     ç¿»è¯‘ {len(text)} å­—ç¬¦å†…å®¹...")
        
        # æ¨¡æ‹Ÿç¿»è¯‘ç»“æœ
        # å®é™…ä½¿ç”¨æ—¶åº”è¯¥è°ƒç”¨DeepSeek API:
        # response = requests.post(
        #     f"{self.deepseek_base_url}/chat/completions",
        #     headers={"Authorization": f"Bearer {self.deepseek_api_key}"},
        #     json={
        #         "model": "deepseek-chat",
        #         "messages": [
        #             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œå°†å¤–æ–‡æ–°é—»å‡†ç¡®ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒåŸæ–‡æ„æ€ä¸å˜ã€‚"},
        #             {"role": "user", "content": f"è¯·å°†ä»¥ä¸‹{source_lang}æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼š\n\n{text}"}
        #         ],
        #         "temperature": 0.3
        #         }
        # )
        
        # æ¨¡æ‹Ÿç¿»è¯‘
        if len(text) > 300:
            return f"[DeepSeekç¿»è¯‘] {text[:300]}..."
        return f"[DeepSeekç¿»è¯‘] {text}"
    
    def get_gnews_headlines(self, category: str = "general", lang: str = "en", country: str = "us") -> List[Dict]:
        """è·å–gnewså¤´æ¡æ–°é—»"""
        params = {
            'category': category,
            'lang': lang,
            'country': country,
            'max': 10
        }
        
        data = self.call_gnews_api("top-headlines", params)
        articles = data.get('articles', [])
        
        processed = []
        for article in articles:
            # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
            content_hash = hashlib.md5(
                f"{article.get('title', '')}{article.get('description', '')}".encode()
            ).hexdigest()
            
            if content_hash in self.seen_articles:
                continue
            
            self.seen_articles.add(content_hash)
            
            processed_article = {
                'title': article.get('title', 'æ— æ ‡é¢˜'),
                'description': article.get('description', ''),
                'content': article.get('content', article.get('description', '')),
                'source': article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº'),
                'published_at': article.get('publishedAt', datetime.now().isoformat()),
                'url': article.get('url', ''),
                'language': lang,
                'needs_translation': lang != 'zh',
                'content_hash': content_hash
            }
            
            # ç¿»è¯‘å¤–æ–‡å†…å®¹
            if processed_article['needs_translation']:
                processed_article['translated_title'] = self.translate_with_deepseek(
                    processed_article['title']
                )
                processed_article['translated_content'] = self.translate_with_deepseek(
                    processed_article['content']
                )
            else:
                processed_article['translated_title'] = processed_article['title']
                processed_article['translated_content'] = processed_article['content']
            
            processed.append(processed_article)
        
        return processed
    
    def search_gnews(self, query: str, lang: str = "en") -> List[Dict]:
        """æœç´¢gnewsæ–°é—»"""
        params = {
            'q': query,
            'lang': lang,
            'max': 10
        }
        
        data = self.call_gnews_api("search", params)
        articles = data.get('articles', [])
        
        processed = []
        for article in articles:
            content_hash = hashlib.md5(
                f"{article.get('title', '')}{article.get('description', '')}".encode()
            ).hexdigest()
            
            if content_hash in self.seen_articles:
                continue
            
            self.seen_articles.add(content_hash)
            
            processed_article = {
                'title': article.get('title', 'æ— æ ‡é¢˜'),
                'description': article.get('description', ''),
                'content': article.get('content', article.get('description', '')),
                'source': article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº'),
                'published_at': article.get('publishedAt', datetime.now().isoformat()),
                'url': article.get('url', ''),
                'language': lang,
                'needs_translation': lang != 'zh',
                'content_hash': content_hash
            }
            
            if processed_article['needs_translation']:
                processed_article['translated_title'] = self.translate_with_deepseek(
                    processed_article['title']
                )
                processed_article['translated_content'] = self.translate_with_deepseek(
                    processed_article['content']
                )
            else:
                processed_article['translated_title'] = processed_article['title']
                processed_article['translated_content'] = processed_article['content']
            
            processed.append(processed_article)
        
        return processed
    
    def get_diverse_news(self) -> Dict:
        """è·å–å¤šæ ·åŒ–æ–°é—»"""
        print("\nğŸš€ å¼€å§‹è·å–å¤šæ ·åŒ–æ–°é—»...")
        print("=" * 60)
        
        all_articles = {
            'foreign_tech': [],
            'foreign_general': [],
            'chinese_news': []
        }
        
        # 1. è·å–è‹±æ–‡ç§‘æŠ€æ–°é—»
        print("ğŸŒ è·å–è‹±æ–‡ç§‘æŠ€æ–°é—»...")
        tech_articles = self.search_gnews("technology", "en")
        all_articles['foreign_tech'] = tech_articles[:5]  # é™åˆ¶5ç¯‡
        print(f"   è·å– {len(tech_articles)} ç¯‡ï¼Œé€‰æ‹© {len(all_articles['foreign_tech'])} ç¯‡")
        
        # 2. è·å–è‹±æ–‡ç»¼åˆæ–°é—»
        print("ğŸŒ è·å–è‹±æ–‡ç»¼åˆæ–°é—»...")
        general_articles = self.get_gnews_headlines("general", "en", "us")
        all_articles['foreign_general'] = general_articles[:5]  # é™åˆ¶5ç¯‡
        print(f"   è·å– {len(general_articles)} ç¯‡ï¼Œé€‰æ‹© {len(all_articles['foreign_general'])} ç¯‡")
        
        # 3. è·å–ä¸­æ–‡æ–°é—»
        print("ğŸ‡¨ğŸ‡³ è·å–ä¸­æ–‡æ–°é—»...")
        chinese_articles = self.get_gnews_headlines("general", "zh", "cn")
        all_articles['chinese_news'] = chinese_articles[:5]  # é™åˆ¶5ç¯‡
        print(f"   è·å– {len(chinese_articles)} ç¯‡ï¼Œé€‰æ‹© {len(all_articles['chinese_news'])} ç¯‡")
        
        # åˆå¹¶æ‰€æœ‰æ–‡ç« 
        all_merged = []
        for category in all_articles.values():
            all_merged.extend(category)
        
        print(f"\nâœ… æ–°é—»è·å–å®Œæˆ!")
        print(f"   æ€»è®¡: {len(all_merged)} ç¯‡æ–‡ç« ")
        print(f"   APIä½¿ç”¨: {self.request_count} æ¬¡")
        
        return {
            'articles': all_merged,
            'stats': {
                'total': len(all_merged),
                'foreign_tech': len(all_articles['foreign_tech']),
                'foreign_general': len(all_articles['foreign_general']),
                'chinese_news': len(all_articles['chinese_news']),
                'api_requests': self.request_count,
                'timestamp': datetime.now().isoformat()
            }
        }
    
    def generate_briefing(self, news_data: Dict) -> str:
        """ç”Ÿæˆç®€æŠ¥"""
        articles = news_data['articles']
        stats = news_data['stats']
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        briefing = f"# ğŸ“° æ–°é—»ç®€æŠ¥ - {timestamp}\n\n"
        briefing += "## ğŸ“Š ç®€æŠ¥æ¦‚è§ˆ\n\n"
        briefing += f"- **æ–°é—»æ€»æ•°**: {stats['total']} ç¯‡\n"
        briefing += f"- **è‹±æ–‡ç§‘æŠ€**: {stats['foreign_tech']} ç¯‡\n"
        briefing += f"- **è‹±æ–‡ç»¼åˆ**: {stats['foreign_general']} ç¯‡\n"
        briefing += f"- **ä¸­æ–‡æ–°é—»**: {stats['chinese_news']} ç¯‡\n"
        briefing += f"- **APIä½¿ç”¨**: {stats['api_requests']} æ¬¡\n"
        briefing += f"- **ç”Ÿæˆæ—¶é—´**: {timestamp}\n\n"
        briefing += "---\n\n"
        
        # è‹±æ–‡ç§‘æŠ€æ–°é—»
        tech_articles = [a for a in articles if a.get('source', '').lower() in ['tech', 'technology', 'wired', 'verge', 'ars'] or 'tech' in a.get('title', '').lower()]
        if tech_articles:
            briefing += "## ğŸ”¬ ç§‘æŠ€å‰æ²¿\n\n"
            for i, article in enumerate(tech_articles[:5], 1):
                briefing += f"### {i}. {article['translated_title']}\n\n"
                briefing += f"**æ¥æº**: {article['source']}\n"
                briefing += f"**å‘å¸ƒæ—¶é—´**: {article['published_at'][:10]}\n\n"
                briefing += f"{article['translated_content'][:200]}...\n\n"
                briefing += "---\n\n"
        
        # è‹±æ–‡ç»¼åˆæ–°é—»
        general_articles = [a for a in articles if a not in tech_articles and a['language'] == 'en']
        if general_articles:
            briefing += "## ğŸŒ å›½é™…è¦é—»\n\n"
            for i, article in enumerate(general_articles[:5], 1):
                briefing += f"### {i}. {article['translated_title']}\n\n"
                briefing += f"**æ¥æº**: {article['source']}\n"
                briefing += f"**å‘å¸ƒæ—¶é—´**: {article['published_at'][:10]}\n\n"
                briefing += f"{article['translated_content'][:200]}...\n\n"
                briefing += "---\n\n"
        
        # ä¸­æ–‡æ–°é—»
        chinese_articles = [a for a in articles if a['language'] == 'zh']
        if chinese_articles:
            briefing += "## ğŸ‡¨ğŸ‡³ å›½å†…åŠ¨æ€\n\n"
            for i, article in enumerate(chinese_articles[:5], 1):
                briefing += f"### {i}. {article['title']}\n\n"
                briefing += f"**æ¥æº**: {article['source']}\n"
                briefing += f"**å‘å¸ƒæ—¶é—´**: {article['published_at'][:10]}\n\n"
                briefing += f"{article['content'][:200]}...\n\n"
                briefing += "---\n\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        briefing += "## ğŸ“ˆ æ•°æ®ç»Ÿè®¡\n\n"
        briefing += f"- **æœ¬æ¬¡ç®€æŠ¥æ–‡ç« æ•°**: {len(articles)} ç¯‡\n"
        briefing += f"- **å¤–æ–‡ç¿»è¯‘**: å…¨éƒ¨ä½¿ç”¨DeepSeek APIç¿»è¯‘\n"
        briefing += f"- **å†…å®¹ç‰¹ç‚¹**: å¤šæ ·åŒ–æ¥æºï¼Œå®æ—¶æ›´æ–°\n"
        briefing += f"- **APIçŠ¶æ€**: {self.request_count}/{self.max_daily_requests} æ¬¡ä½¿ç”¨\n\n"
        
        briefing += "---\n\n"
        briefing += "*æœ¬ç®€æŠ¥åŸºäºgnews.io APIç”Ÿæˆï¼Œå¤–æ–‡å†…å®¹ç»DeepSeekç¿»è¯‘*\n"
        briefing += "*ä¸“æ³¨æ–°é—»å†…å®¹ï¼ŒåŸæ–‡é“¾æ¥å·²éšè—*\n"
        briefing += "*æ¯æ—¥ä¸‰æ¬¡æ›´æ–°ï¼Œæ¯æ¬¡10-15ç¯‡ç²¾é€‰æ–°é—»*\n"
        
        return briefing
    
    def save_results(self, news_data: Dict, briefing: str):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜æ•°æ®
        data_file = f"data/gnews_briefings/gnews_data_{timestamp}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç®€æŠ¥
        briefing_file = f"data/gnews_briefings/briefing_{timestamp}.md"
        with open(briefing_file, 'w', encoding='utf-8') as f:
            f.write(briefing)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"   ç®€æŠ¥æ–‡ä»¶: {briefing_file}")
        
        return briefing_file
    
    def run(self):
        """è¿è¡Œç³»ç»Ÿ"""
        print("=" * 60)
        print("ğŸ“° gnews.ioé›†æˆç‰ˆæ–°é—»çˆ¬å–ç³»ç»Ÿ")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # è·å–æ–°é—»
            news_data = self.get_diverse_news()
            
            if not news_data['articles']:
                print("âŒ æœªè·å–åˆ°æ–°é—»å†…å®¹")
                return None
            
            # ç”Ÿæˆç®€æŠ¥
            briefing = self.generate_briefing(news_data)
            
            # ä¿å­˜ç»“æœ
            briefing_file = self.save_results(news_data, briefing)
            
            elapsed_time = time.time() - start_time
            
            print(f"\nğŸ“„ ç®€æŠ¥é¢„è§ˆ:")
            print("=" * 60)
            # æ˜¾ç¤ºå‰15è¡Œ
            lines = briefing.split('\n')[:15]
            for line in lines:
                print(f"   {line}")
            print("   ...")
            print("=" * 60)
            
            print(f"\nâœ… ç³»ç»Ÿè¿è¡Œå®Œæˆ!")
            print(f"   è€—æ—¶: {elapsed_time:.1f}ç§’")
            print(f"   è·å–æ–°é—»: {len(news_data['articles'])} ç¯‡")
            print(f"   APIä½¿ç”¨: {self.request_count} æ¬¡")
            print(f"   ç®€æŠ¥æ–‡ä»¶: {briefing_file}")
            
            return briefing_file
            
        except Exception as e:
            print(f"\nâŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    crawler = GNewsIntegratedCrawler()
    crawler.run()

if __name__ == "__main__":
    main()