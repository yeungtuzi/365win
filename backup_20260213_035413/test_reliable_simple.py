#!/usr/bin/env python3
# ç®€å•æµ‹è¯•å¯é çˆ¬è™«

import os
import sys
import json
import time
from datetime import datetime

print("ğŸ§ª ç®€å•æµ‹è¯•å¯é çˆ¬è™«")
print("=" * 50)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    # 1. æµ‹è¯•å¯¼å…¥
    print("1. æµ‹è¯•å¯¼å…¥...")
    from scripts.reliable_crawler import ReliableCrawler
    print("âœ… å¯é çˆ¬è™«å¯¼å…¥æˆåŠŸ")
    
    # 2. åˆå§‹åŒ–
    print("\n2. åˆå§‹åŒ–çˆ¬è™«...")
    crawler = ReliableCrawler()
    print("âœ… å¯é çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
    
    # 3. æµ‹è¯•å•ä¸ªæºï¼ˆHacker Newsï¼‰
    print("\n3. æµ‹è¯•Hacker News API...")
    import requests
    
    hn_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
    response = requests.get(hn_url, timeout=10)
    
    if response.status_code == 200:
        top_stories = response.json()[:3]
        print(f"âœ… Hacker News APIå·¥ä½œæ­£å¸¸ï¼Œè·å–åˆ° {len(top_stories)} ä¸ªæ•…äº‹ID")
        
        # è·å–ç¬¬ä¸€ä¸ªæ•…äº‹çš„è¯¦æƒ…
        if top_stories:
            story_id = top_stories[0]
            story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            story_response = requests.get(story_url, timeout=10)
            
            if story_response.status_code == 200:
                story = story_response.json()
                print(f"   æ•…äº‹ç¤ºä¾‹: {story.get('title', 'æ— æ ‡é¢˜')[:50]}...")
                print(f"   æ¥æº: Hacker News")
                print(f"   URL: {story.get('url', 'æ— URL')}")
            else:
                print(f"   âš ï¸ è·å–æ•…äº‹è¯¦æƒ…å¤±è´¥: {story_response.status_code}")
    else:
        print(f"âŒ Hacker News APIå¤±è´¥: {response.status_code}")
    
    # 4. æµ‹è¯•RSSæº
    print("\n4. æµ‹è¯•RSSæº...")
    try:
        import feedparser
        print("âœ… feedparserå¯ç”¨")
        
        # æµ‹è¯•Ars Technica RSS
        rss_url = "https://feeds.arstechnica.com/arstechnica/index"
        feed = feedparser.parse(rss_url)
        
        if feed.entries:
            print(f"âœ… RSSè§£ææˆåŠŸï¼Œè·å–åˆ° {len(feed.entries)} ä¸ªæ¡ç›®")
            
            for i, entry in enumerate(feed.entries[:2]):
                print(f"   æ¡ç›® {i+1}: {entry.get('title', 'æ— æ ‡é¢˜')[:50]}...")
                print(f"       é“¾æ¥: {entry.get('link', 'æ— é“¾æ¥')}")
                print(f"       æ‘˜è¦: {entry.get('summary', 'æ— æ‘˜è¦')[:80]}...")
        else:
            print("âš ï¸ RSSè§£æè¿”å›0ä¸ªæ¡ç›®")
            
    except Exception as e:
        print(f"âŒ RSSæµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
    
    # 5. æµ‹è¯•çˆ¬å–
    print("\n5. æµ‹è¯•å®Œæ•´çˆ¬å–...")
    start_time = time.time()
    
    try:
        result = crawler.daily_crawl()
        elapsed = time.time() - start_time
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
        print(f"   å¤–æ–‡æ–‡ç« : {len(result['foreign'])} ç¯‡")
        print(f"   ä¸­æ–‡æ–‡ç« : {len(result['chinese'])} ç¯‡")
        
        total_articles = len(result['foreign']) + len(result['chinese'])
        
        if total_articles > 0:
            print(f"\nğŸ‰ æˆåŠŸè·å– {total_articles} ç¯‡æ–‡ç« !")
            
            # æ˜¾ç¤ºç¤ºä¾‹
            if result['foreign']:
                print("\n   å¤–æ–‡æ–‡ç« ç¤ºä¾‹:")
                for i, article in enumerate(result['foreign'][:2]):
                    print(f"     {i+1}. {article['title'][:50]}...")
                    print(f"         æ¥æº: {article['source']}")
                    print(f"         å†…å®¹é•¿åº¦: {len(article['content'])} å­—ç¬¦")
            
            if result['chinese']:
                print("\n   ä¸­æ–‡æ–‡ç« ç¤ºä¾‹:")
                for i, article in enumerate(result['chinese'][:2]):
                    print(f"     {i+1}. {article['title'][:50]}...")
                    print(f"         æ¥æº: {article['source']}")
                    print(f"         å†…å®¹é•¿åº¦: {len(article['content'])} å­—ç¬¦")
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            print("\n6. ä¿å­˜æµ‹è¯•ç»“æœ...")
            test_dir = "data/reliable_tests"
            os.makedirs(test_dir, exist_ok=True)
            
            test_result = {
                "timestamp": datetime.now().isoformat(),
                "test_name": "å¯é çˆ¬è™«æµ‹è¯•",
                "foreign_articles": len(result['foreign']),
                "chinese_articles": len(result['chinese']),
                "total_articles": total_articles,
                "elapsed_seconds": elapsed,
                "sources_tested": [
                    "Hacker News",
                    "Reddit r/technology", 
                    "Ars Technica RSS",
                    "çŸ¥ä¹çƒ­æ¦œ",
                    "æ¾æ¹ƒæ–°é—»",
                    "æœå£³ç½‘"
                ],
                "status": "æˆåŠŸ" if total_articles > 0 else "éƒ¨åˆ†æˆåŠŸ",
                "recommendation": "çˆ¬è™«å·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥é›†æˆåˆ°ä¸»ç³»ç»Ÿ" if total_articles > 0 else "éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•"
            }
            
            result_file = f"{test_dir}/reliable_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(test_result, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            
            print("\n" + "=" * 50)
            if total_articles > 0:
                print("ğŸ‰ ğŸ‰ ğŸ‰ å¯é çˆ¬è™«æµ‹è¯•æˆåŠŸ!")
                print("âœ… çœŸå®å†…å®¹è·å–åŠŸèƒ½æ­£å¸¸")
                print("âœ… å¯ä»¥é›†æˆåˆ°æ··åˆçˆ¬è™«ç³»ç»Ÿ")
                print("ğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½éƒ¨ç½²!")
            else:
                print("âš ï¸ çˆ¬è™«è¿”å›0ç¯‡æ–‡ç« ")
                print("å»ºè®®ä½¿ç”¨æ··åˆç³»ç»Ÿï¼ˆæ¨¡æ‹Ÿæ•°æ®+å°è¯•çœŸå®çˆ¬å–ï¼‰")
            print("=" * 50)
            
        else:
            print("âŒ çˆ¬å–è¿”å›0ç¯‡æ–‡ç« ")
            print("å»ºè®®:")
            print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("  2. æŸäº›APIå¯èƒ½è¢«é™åˆ¶")
            print("  3. ä½¿ç”¨æ··åˆç³»ç»Ÿç¡®ä¿å†…å®¹å¯ç”¨")
            
    except Exception as e:
        print(f"âŒ çˆ¬å–æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥ä¾èµ–å®‰è£…: pip3 install feedparser beautifulsoup4")
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()