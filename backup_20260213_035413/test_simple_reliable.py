#!/usr/bin/env python3
# ç®€å•æµ‹è¯•å¯é çˆ¬è™«

import os
import sys
import json
from datetime import datetime

print("ğŸ§ª æµ‹è¯•å¯é çˆ¬è™«ï¼ˆç®€åŒ–ç‰ˆï¼‰")
print("=" * 50)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    # æµ‹è¯•æ··åˆçˆ¬è™«
    print("1. æµ‹è¯•æ··åˆçˆ¬è™«...")
    from scripts.hybrid_crawler import HybridCrawler
    
    crawler = HybridCrawler()
    print("âœ… æ··åˆçˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
    
    # è·å–æ•°æ®æºä¿¡æ¯
    source_info = crawler.get_data_source_info()
    print(f"   æ•°æ®æºçŠ¶æ€:")
    print(f"     - çœŸå®çˆ¬è™«å¯ç”¨: {source_info['real_crawler_available']}")
    print(f"     - å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {source_info['using_mock_data']}")
    print(f"     - æ¨¡æ‹Ÿæ•°æ®è´¨é‡: {source_info['mock_data_quality']}")
    
    # è·å–å¤„ç†å†…å®¹
    print("\n2. è·å–å¤„ç†å†…å®¹...")
    articles = crawler.get_content_for_processing(use_cached=True)
    
    if articles:
        print(f"âœ… è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        foreign = [a for a in articles if a["needs_translation"]]
        chinese = [a for a in articles if not a["needs_translation"]]
        
        print(f"   éœ€è¦ç¿»è¯‘ï¼ˆå¤–æ–‡ï¼‰: {len(foreign)} ç¯‡")
        print(f"   ä¸­æ–‡åŸæ–‡: {len(chinese)} ç¯‡")
        
        print("\n   æ–‡ç« ç¤ºä¾‹:")
        for i, article in enumerate(articles[:3]):
            lang = "å¤–æ–‡" if article["needs_translation"] else "ä¸­æ–‡"
            data_source = "æ¨¡æ‹Ÿ" if crawler.use_mock_data else "çœŸå®"
            print(f"   {i+1}. [{data_source}][{lang}][{article['source']}]")
            print(f"       æ ‡é¢˜: {article['title'][:50]}...")
            print(f"       å†…å®¹é•¿åº¦: {len(article['content'])} å­—ç¬¦")
            print(f"       æ‘˜è¦: {article['content'][:80]}...")
            print()
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°æ–‡ç« ")
    
    # æµ‹è¯•ä¸»å·¥ä½œæµ
    print("\n3. æµ‹è¯•ä¸»å·¥ä½œæµ...")
    from scripts.main_workflow import Year365WinWorkflow
    
    workflow = Year365WinWorkflow()
    print("âœ… ä¸»å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ")
    
    # æµ‹è¯•ç®€æŠ¥ç”Ÿæˆ
    print("\n4. ç”Ÿæˆç®€æŠ¥...")
    briefing = workflow.run_daily_workflow("morning", use_cached=True)
    
    if briefing:
        print(f"âœ… ç®€æŠ¥ç”ŸæˆæˆåŠŸ!")
        print(f"   ç®€æŠ¥é•¿åº¦: {len(briefing)} å­—ç¬¦")
        
        # æ˜¾ç¤ºç®€æŠ¥å¼€å¤´
        print("\n   ç®€æŠ¥é¢„è§ˆ:")
        lines = briefing.split('\n')[:8]
        for line in lines:
            if line.strip():
                print(f"    {line}")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        print("\n5. ä¿å­˜æµ‹è¯•ç»“æœ...")
        test_dir = "data/simple_tests"
        os.makedirs(test_dir, exist_ok=True)
        
        test_result = {
            "timestamp": datetime.now().isoformat(),
            "test_name": "ç®€å•å¯é æµ‹è¯•",
            "articles_count": len(articles) if 'articles' in locals() else 0,
            "briefing_generated": briefing is not None,
            "using_mock_data": source_info.get('using_mock_data', True),
            "system_status": "åŠŸèƒ½æ­£å¸¸"
        }
        
        result_file = f"{test_dir}/simple_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        
        print("\n" + "=" * 50)
        print("ğŸ‰ ç®€å•æµ‹è¯•å®Œæˆ!")
        print("âœ… æ··åˆçˆ¬è™«ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        print("âœ… ä¸»å·¥ä½œæµç”Ÿæˆç®€æŠ¥æˆåŠŸ")
        print("âœ… ç³»ç»Ÿå·²å‡†å¤‡å¥½éƒ¨ç½²")
        print("=" * 50)
        
    else:
        print("âŒ ç®€æŠ¥ç”Ÿæˆå¤±è´¥")
        
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥ä¾èµ–å®‰è£…")
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()