#!/usr/bin/env python3
# æµ‹è¯•çœŸå®çˆ¬å–åŠŸèƒ½

import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from scripts.web_crawler import WebCrawler
from scripts.deepseek_client import DeepSeekClient

print("ğŸš€ æµ‹è¯•çœŸå®çˆ¬å–åŠŸèƒ½")
print("=" * 60)

# 1. æµ‹è¯•çˆ¬è™«
print("\n1ï¸âƒ£ æµ‹è¯•ç½‘ç»œçˆ¬è™«...")
crawler = WebCrawler("cache/raw_data")

# å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
print("å°è¯•ä»ç¼“å­˜åŠ è½½æ•°æ®...")
cached_data = crawler.load_from_cache(hours=72)

if cached_data:
    print(f"âœ… ä»ç¼“å­˜åŠ è½½æˆåŠŸ:")
    print(f"   æ—¶é—´: {cached_data['timestamp']}")
    print(f"   å¤–æ–‡: {cached_data['total_foreign']}æ¡")
    print(f"   ä¸­æ–‡: {cached_data['total_chinese']}æ¡")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    print("\nğŸ“° ç¼“å­˜å†…å®¹ç¤ºä¾‹:")
    foreign_items = cached_data['items']['foreign'][:2]
    chinese_items = cached_data['items']['chinese'][:2]
    
    for i, item in enumerate(foreign_items):
        print(f"  å¤–æ–‡{i+1}: [{item['source']}] {item['title'][:60]}...")
    
    for i, item in enumerate(chinese_items):
        print(f"  ä¸­æ–‡{i+1}: [{item['source']}] {item['title'][:60]}...")
else:
    print("âŒ æ— æœ‰æ•ˆç¼“å­˜ï¼Œéœ€è¦å®æ—¶çˆ¬å–")

# 2. æµ‹è¯•å®æ—¶çˆ¬å–ï¼ˆå¯é€‰ï¼‰
print("\n2ï¸âƒ£ æµ‹è¯•å®æ—¶çˆ¬å–ï¼ˆæŒ‰Enterè·³è¿‡ï¼Œè¾“å…¥yå¼€å§‹ï¼‰:")
choice = input("æ˜¯å¦å¼€å§‹å®æ—¶çˆ¬å–? (y/N): ")

if choice.lower() == 'y':
    print("å¼€å§‹å®æ—¶çˆ¬å–...")
    crawled_data = crawler.crawl_all_sources(max_items_per_source=3)
    
    print(f"âœ… çˆ¬å–å®Œæˆ:")
    print(f"   å¤–æ–‡: {len(crawled_data['foreign'])}æ¡")
    print(f"   ä¸­æ–‡: {len(crawled_data['chinese'])}æ¡")
    
    print("\nğŸ“° æœ€æ–°å†…å®¹ç¤ºä¾‹:")
    for i, item in enumerate(crawled_data['foreign'][:3]):
        print(f"  å¤–æ–‡{i+1}: [{item['source']}] {item['title'][:60]}...")
    
    for i, item in enumerate(crawled_data['chinese'][:3]):
        print(f"  ä¸­æ–‡{i+1}: [{item['source']}] {item['title'][:60]}...")

# 3. æµ‹è¯•æ··åˆå†…å®¹è·å–
print("\n3ï¸âƒ£ æµ‹è¯•æ··åˆå†…å®¹è·å–ï¼ˆ70%å¤–æ–‡ + 30%ä¸­æ–‡ï¼‰...")
mixed_items = crawler.get_content_for_recommendation(use_cached=True)

print(f"âœ… è·å–åˆ° {len(mixed_items)} æ¡æ··åˆå†…å®¹:")
print(f"   éœ€è¦ç¿»è¯‘: {sum(1 for item in mixed_items if item.get('needs_translation'))}æ¡")
print(f"   ä¸­æ–‡åŸæ–‡: {sum(1 for item in mixed_items if not item.get('needs_translation'))}æ¡")

print("\nğŸ“‹ å†…å®¹è¯¦æƒ…:")
for i, item in enumerate(mixed_items[:5]):
    lang = "å¤–æ–‡" if item.get('needs_translation') else "ä¸­æ–‡"
    print(f"  {i+1}. [{lang}][{item['source']}] {item['title'][:50]}...")

# 4. æµ‹è¯•DeepSeekç¿»è¯‘ï¼ˆå¦‚æœé…ç½®äº†APIå¯†é’¥ï¼‰
print("\n4ï¸âƒ£ æµ‹è¯•DeepSeekç¿»è¯‘åŠŸèƒ½...")
api_key = os.getenv("DEEPSEEK_API_KEY")
if api_key and api_key != "test_mode_key":
    print("æ£€æµ‹åˆ°DeepSeek APIå¯†é’¥ï¼Œæµ‹è¯•ç¿»è¯‘...")
    
    try:
        client = DeepSeekClient(api_key)
        
        # æ‰¾ä¸€ä¸ªéœ€è¦ç¿»è¯‘çš„å¤–æ–‡å†…å®¹
        foreign_item = None
        for item in mixed_items:
            if item.get('needs_translation'):
                foreign_item = item
                break
        
        if foreign_item:
            print(f"ç¿»è¯‘æµ‹è¯•: {foreign_item['title'][:30]}...")
            translated = client.translate_content(foreign_item['title'], target_lang="zh")
            print(f"âœ… ç¿»è¯‘ç»“æœ: {translated}")
        else:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç¿»è¯‘çš„å†…å®¹")
            
    except Exception as e:
        print(f"âŒ ç¿»è¯‘æµ‹è¯•å¤±è´¥: {e}")
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„DeepSeek APIå¯†é’¥ï¼Œè·³è¿‡ç¿»è¯‘æµ‹è¯•")

# 5. æµ‹è¯•ç¼“å­˜æ¸…ç†
print("\n5ï¸âƒ£ æµ‹è¯•ç¼“å­˜æ¸…ç†...")
crawler.clean_old_cache(days=1)
print("âœ… ç¼“å­˜æ¸…ç†å®Œæˆï¼ˆæ¸…ç†1å¤©å‰çš„ç¼“å­˜ï¼‰")

# 6. ä¿å­˜æµ‹è¯•ç»“æœ
print("\n6ï¸âƒ£ ä¿å­˜æµ‹è¯•ç»“æœ...")
test_dir = "data/test_results"
os.makedirs(test_dir, exist_ok=True)

test_file = f"{test_dir}/crawler_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

import json
test_result = {
    "timestamp": datetime.now().isoformat(),
    "cached_available": cached_data is not None,
    "mixed_items_count": len(mixed_items),
    "needs_translation": sum(1 for item in mixed_items if item.get('needs_translation')),
    "sample_items": mixed_items[:10]  # ä¿å­˜å‰10æ¡ä½œä¸ºç¤ºä¾‹
}

with open(test_file, 'w', encoding='utf-8') as f:
    json.dump(test_result, f, ensure_ascii=False, indent=2)

print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_file}")

print("\n" + "=" * 60)
print("ğŸ‰ çœŸå®çˆ¬å–åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
print("âœ¨ ç³»ç»Ÿç°åœ¨å¯ä»¥ä»äº’è”ç½‘å®æ—¶è·å–ä¸­å¤–å†…å®¹")
print("ğŸŒ æ”¯æŒ70%å¤–æ–‡ + 30%ä¸­æ–‡çš„æ··åˆæ¨è")
print("ğŸ’¾ æ”¯æŒ3å¤©ç¼“å­˜ï¼Œå®ç°'æ¢ä¸€æ‰¹'åŠŸèƒ½")
print("=" * 60)