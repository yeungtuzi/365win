#!/usr/bin/env python3
# åªæµ‹è¯•çˆ¬è™«åŠŸèƒ½

import os
import sys
from datetime import datetime

print("ğŸ•·ï¸ æµ‹è¯•å®Œæ•´å†…å®¹çˆ¬è™«åŠŸèƒ½")
print("=" * 50)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from scripts.full_content_crawler import FullContentCrawler

try:
    # 1. åˆå§‹åŒ–çˆ¬è™«
    print("1. åˆå§‹åŒ–å®Œæ•´å†…å®¹çˆ¬è™«...")
    crawler = FullContentCrawler()
    print("âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
    
    # 2. æ£€æŸ¥ç°æœ‰æ•°æ®
    print("\n2. æ£€æŸ¥ç°æœ‰æ•°æ®...")
    articles = crawler.load_recent_data(days=3)
    
    if articles:
        print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡ç°æœ‰æ–‡ç« ")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        foreign = [a for a in articles if a["language"] == "en"]
        chinese = [a for a in articles if a["language"] == "zh"]
        
        print(f"   å¤–æ–‡æ–‡ç« : {len(foreign)} ç¯‡")
        print(f"   ä¸­æ–‡æ–‡ç« : {len(chinese)} ç¯‡")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        if articles:
            print("\n   æ–‡ç« ç¤ºä¾‹:")
            for i, article in enumerate(articles[:2]):
                lang = "å¤–æ–‡" if article["language"] == "en" else "ä¸­æ–‡"
                print(f"   {i+1}. [{lang}][{article['source']}]")
                print(f"      æ ‡é¢˜: {article['title'][:50]}...")
                print(f"      å†…å®¹é•¿åº¦: {len(article['content'])} å­—ç¬¦")
                print(f"      æ‘˜è¦: {article['content'][:80]}...")
    
    else:
        print("âš ï¸ æ²¡æœ‰ç°æœ‰æ•°æ®ï¼Œéœ€è¦æ‰§è¡Œçˆ¬å–")
        
        # 3. æµ‹è¯•çˆ¬å–ï¼ˆå¯é€‰ï¼‰
        print("\n3. æ˜¯å¦æ‰§è¡Œæµ‹è¯•çˆ¬å–ï¼Ÿ")
        print("   æ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œå¹¶ä¸”éœ€è¦ç½‘ç»œè¿æ¥")
        print("   è¾“å…¥ 'y' å¼€å§‹çˆ¬å–ï¼Œå…¶ä»–é”®è·³è¿‡...")
        
        import select
        import sys
        
        # éé˜»å¡è¾“å…¥æ£€æŸ¥
        i, o, e = select.select([sys.stdin], [], [], 5)
        
        if i:
            choice = sys.stdin.readline().strip().lower()
        else:
            choice = 'n'
            print("   è¶…æ—¶ï¼Œè·³è¿‡çˆ¬å–æµ‹è¯•")
        
        if choice == 'y':
            print("\nå¼€å§‹æµ‹è¯•çˆ¬å–...")
            try:
                result = crawler.daily_crawl()
                print(f"âœ… çˆ¬å–å®Œæˆ!")
                print(f"   å¤–æ–‡æ–‡ç« : {len(result['foreign'])} ç¯‡")
                print(f"   ä¸­æ–‡æ–‡ç« : {len(result['chinese'])} ç¯‡")
                
                if result['foreign']:
                    print("\n   å¤–æ–‡æ–‡ç« ç¤ºä¾‹:")
                    for i, article in enumerate(result['foreign'][:2]):
                        print(f"     {i+1}. {article['title'][:50]}...")
                        print(f"        å†…å®¹: {article['content'][:80]}...")
                
                if result['chinese']:
                    print("\n   ä¸­æ–‡æ–‡ç« ç¤ºä¾‹:")
                    for i, article in enumerate(result['chinese'][:2]):
                        print(f"     {i+1}. {article['title'][:50]}...")
                        print(f"        å†…å®¹: {article['content'][:80]}...")
                        
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
                print("å»ºè®®ï¼š")
                print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                print("  2. ç½‘ç«™å¯èƒ½é™åˆ¶äº†çˆ¬å–")
                print("  3. å¯ä»¥ç¨åé‡è¯•æˆ–ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        else:
            print("è·³è¿‡çˆ¬å–æµ‹è¯•")
    
    # 4. æµ‹è¯•è·å–å¤„ç†å†…å®¹
    print("\n4. æµ‹è¯•è·å–å¤„ç†å†…å®¹...")
    process_content = crawler.get_content_for_processing(use_cached=True)
    
    if process_content:
        print(f"âœ… è·å–åˆ° {len(process_content)} ç¯‡å¤„ç†å†…å®¹")
        
        foreign = [a for a in process_content if a["needs_translation"]]
        chinese = [a for a in process_content if not a["needs_translation"]]
        
        print(f"   éœ€è¦ç¿»è¯‘ï¼ˆå¤–æ–‡ï¼‰: {len(foreign)} ç¯‡")
        print(f"   ä¸­æ–‡åŸæ–‡: {len(chinese)} ç¯‡")
        
        print("\n   å¤„ç†å†…å®¹ç¤ºä¾‹:")
        for i, article in enumerate(process_content[:2]):
            lang = "å¤–æ–‡" if article["needs_translation"] else "ä¸­æ–‡"
            print(f"   {i+1}. [{lang}][{article['source']}] {article['title'][:40]}...")
            print(f"      å†…å®¹é•¿åº¦: {len(article['content'])} å­—ç¬¦")
    
    else:
        print("âš ï¸ æ²¡æœ‰å¯å¤„ç†çš„å†…å®¹")
        print("å»ºè®®æ‰§è¡Œçˆ¬å–ä»»åŠ¡æˆ–ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    
    # 5. ä¿å­˜æµ‹è¯•ç»“æœ
    print("\n5. ä¿å­˜æµ‹è¯•ç»“æœ...")
    test_dir = "data/crawl_tests"
    os.makedirs(test_dir, exist_ok=True)
    
    result = {
        "timestamp": datetime.now().isoformat(),
        "existing_articles": len(articles) if 'articles' in locals() else 0,
        "process_content": len(process_content) if 'process_content' in locals() else 0,
        "crawl_performed": 'result' in locals(),
        "system_status": "çˆ¬è™«åŠŸèƒ½æµ‹è¯•å®Œæˆ"
    }
    
    result_file = f"{test_dir}/crawl_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        import json
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    print("\n" + "=" * 50)
    print("ğŸ“Š çˆ¬è™«åŠŸèƒ½æµ‹è¯•æ€»ç»“:")
    print("   1. âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
    print(f"   2. ğŸ“ ç°æœ‰æ•°æ®: {len(articles) if 'articles' in locals() else 0} ç¯‡")
    print(f"   3. ğŸ”„ å¯å¤„ç†å†…å®¹: {len(process_content) if 'process_content' in locals() else 0} ç¯‡")
    print("   4. âš™ï¸ ç³»ç»Ÿå°±ç»ª: éœ€è¦æ•°æ®æ‰èƒ½è¿è¡Œå®Œæ•´æµç¨‹")
    print("=" * 50)
    
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œä¾èµ–")
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()