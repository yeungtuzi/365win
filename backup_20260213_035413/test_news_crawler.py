#!/usr/bin/env python3
# æµ‹è¯•æ–°é—»çˆ¬å–ç³»ç»Ÿ

import os
import sys
sys.path.append('.')

print("ğŸ§ª æµ‹è¯•æ–°é—»çˆ¬å–ç³»ç»Ÿ")
print("=" * 60)

# æµ‹è¯•é…ç½®åŠ è½½
print("1. æµ‹è¯•é…ç½®åŠ è½½...")
try:
    import yaml
    with open('config/news_crawler_config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print(f"   âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"   æ•°æ®æº: {len(config['crawler']['sources']['foreign'])}å¤–æ–‡ + {len(config['crawler']['sources']['chinese'])}ä¸­æ–‡")
    print(f"   å»é‡: {config['crawler']['deduplication']['enabled']}")
    print(f"   ç¿»è¯‘: {config['processing']['translation']['enabled']}")
    print(f"   é£æ ¼é‡å†™: {config['processing']['translation'].get('style_rewriting', False)}")
    
except Exception as e:
    print(f"   âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")

# æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–
print("\n2. æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–...")
try:
    from news_crawler_system import NewsCrawlerSystem
    
    crawler = NewsCrawlerSystem()
    print("   âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    
    # æµ‹è¯•å»é‡åŠŸèƒ½
    print("\n3. æµ‹è¯•å»é‡åŠŸèƒ½...")
    test_articles = [
        {
            'title': 'æµ‹è¯•æ–‡ç« 1',
            'content': 'è¿™æ˜¯æµ‹è¯•å†…å®¹1',
            'url': 'https://example.com/1',
            'source': 'æµ‹è¯•æº',
            'language': 'zh',
            'publish_date': '2026-02-13'
        },
        {
            'title': 'æµ‹è¯•æ–‡ç« 1',  # ç›¸åŒæ ‡é¢˜
            'content': 'è¿™æ˜¯æµ‹è¯•å†…å®¹1',  # ç›¸åŒå†…å®¹
            'url': 'https://example.com/2',  # ä¸åŒURL
            'source': 'æµ‹è¯•æº',
            'language': 'zh',
            'publish_date': '2026-02-13'
        }
    ]
    
    duplicate_count = 0
    for article in test_articles:
        if crawler.is_duplicate(article):
            duplicate_count += 1
            print(f"   æ£€æµ‹åˆ°é‡å¤: {article['title']}")
        else:
            print(f"   éé‡å¤: {article['title']}")
    
    print(f"   å»é‡æµ‹è¯•: {duplicate_count}/{len(test_articles)} ç¯‡è¢«è¯†åˆ«ä¸ºé‡å¤")
    
    # æµ‹è¯•å†…å®¹å“ˆå¸Œ
    print("\n4. æµ‹è¯•å†…å®¹å“ˆå¸Œ...")
    content1 = "è¿™æ˜¯æµ‹è¯•å†…å®¹"
    content2 = "è¿™æ˜¯æµ‹è¯•å†…å®¹"  # å®Œå…¨ç›¸åŒ
    content3 = "è¿™æ˜¯ä¸åŒçš„æµ‹è¯•å†…å®¹"
    
    hash1 = crawler.calculate_content_hash(content1)
    hash2 = crawler.calculate_content_hash(content2)
    hash3 = crawler.calculate_content_hash(content3)
    
    print(f"   å†…å®¹1å“ˆå¸Œ: {hash1[:8]}...")
    print(f"   å†…å®¹2å“ˆå¸Œ: {hash2[:8]}... (åº”ä¸å†…å®¹1ç›¸åŒ)")
    print(f"   å†…å®¹3å“ˆå¸Œ: {hash3[:8]}... (åº”ä¸åŒ)")
    
    if hash1 == hash2:
        print("   âœ… ç›¸åŒå†…å®¹å“ˆå¸Œä¸€è‡´")
    else:
        print("   âŒ ç›¸åŒå†…å®¹å“ˆå¸Œä¸ä¸€è‡´")
    
    if hash1 != hash3:
        print("   âœ… ä¸åŒå†…å®¹å“ˆå¸Œä¸åŒ")
    else:
        print("   âŒ ä¸åŒå†…å®¹å“ˆå¸Œç›¸åŒ")
    
    # æµ‹è¯•æ‘˜è¦ç”Ÿæˆ
    print("\n5. æµ‹è¯•æ‘˜è¦ç”Ÿæˆ...")
    long_content = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æµ‹è¯•å†…å®¹ï¼Œéœ€è¦è¢«æˆªæ–­æˆæ‘˜è¦ã€‚" * 10
    summary = crawler.generate_summary(long_content, max_length=50)
    
    print(f"   åŸå§‹å†…å®¹é•¿åº¦: {len(long_content)} å­—ç¬¦")
    print(f"   æ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
    print(f"   æ‘˜è¦å†…å®¹: {summary}")
    
    if len(summary) <= 50 + 3:  # 50å­—ç¬¦ + "..."
        print("   âœ… æ‘˜è¦ç”Ÿæˆæ­£ç¡®")
    else:
        print("   âŒ æ‘˜è¦è¿‡é•¿")
    
    # æµ‹è¯•ç®€æŠ¥ç”Ÿæˆ
    print("\n6. æµ‹è¯•ç®€æŠ¥ç”Ÿæˆ...")
    test_data = {
        'foreign': [
            {
                'title': 'å¤–æ–‡æµ‹è¯•æ–‡ç« 1',
                'content': 'è¿™æ˜¯å¤–æ–‡æµ‹è¯•å†…å®¹1',
                'url': 'https://foreign.com/1',
                'source': 'Reuters',
                'language': 'en',
                'publish_date': '2026-02-13T10:00:00',
                'needs_translation': True,
                'translated_content': '[ç¿»è¯‘è‡ªen] è¿™æ˜¯å¤–æ–‡æµ‹è¯•å†…å®¹1',
                'summary': 'å¤–æ–‡æµ‹è¯•æ‘˜è¦1'
            }
        ],
        'chinese': [
            {
                'title': 'ä¸­æ–‡æµ‹è¯•æ–‡ç« 1',
                'content': 'è¿™æ˜¯ä¸­æ–‡æµ‹è¯•å†…å®¹1',
                'url': 'https://chinese.com/1',
                'source': 'æ¾æ¹ƒæ–°é—»',
                'language': 'zh',
                'publish_date': '2026-02-13T11:00:00',
                'needs_translation': False,
                'translated_content': 'è¿™æ˜¯ä¸­æ–‡æµ‹è¯•å†…å®¹1',
                'summary': 'ä¸­æ–‡æµ‹è¯•æ‘˜è¦1'
            }
        ]
    }
    
    briefing = crawler.generate_briefing(test_data)
    
    print(f"   ç®€æŠ¥é•¿åº¦: {len(briefing)} å­—ç¬¦")
    print(f"   åŒ…å«å¤–æ–‡æ–°é—»: {'å¤–æ–‡æ–°é—»' in briefing}")
    print(f"   åŒ…å«ä¸­æ–‡æ–°é—»: {'ä¸­æ–‡æ–°é—»' in briefing}")
    print(f"   åŒ…å«ç»Ÿè®¡ä¿¡æ¯: {'ç»Ÿè®¡ä¿¡æ¯' in briefing}")
    
    # æ˜¾ç¤ºç®€æŠ¥å¼€å¤´
    print("\n   ç®€æŠ¥é¢„è§ˆ:")
    print("   " + "-" * 40)
    for line in briefing.split('\n')[:10]:
        print(f"   {line}")
    print("   " + "-" * 40)
    
    print("\n" + "=" * 60)
    print("âœ… æ–°é—»çˆ¬å–ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
    print("   æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡")
    print("=" * 60)
    
except Exception as e:
    print(f"   âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()

print("\nğŸš€ å‡†å¤‡è¿è¡Œå®Œæ•´çš„æ–°é—»çˆ¬å–ç³»ç»Ÿ...")
print("è¾“å…¥ 'y' å¼€å§‹è¿è¡Œï¼Œæˆ–æŒ‰å…¶ä»–é”®è·³è¿‡:")
choice = input().strip().lower()

if choice == 'y':
    print("\n" + "=" * 60)
    print("å¼€å§‹è¿è¡Œæ–°é—»çˆ¬å–ç³»ç»Ÿ...")
    print("=" * 60)
    
    # è¿è¡Œå®Œæ•´ç³»ç»Ÿ
    crawler.run()
else:
    print("\nè·³è¿‡å®Œæ•´è¿è¡Œã€‚")
    print("è¦è¿è¡Œå®Œæ•´ç³»ç»Ÿï¼Œè¯·æ‰§è¡Œ:")
    print("  python3 news_crawler_system.py")